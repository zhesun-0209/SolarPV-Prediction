#!/usr/bin/env python3
"""
è¿è¡ŒProject1140æ¶ˆèå®éªŒ
æ‰§è¡Œæ‰€æœ‰360ä¸ªå®éªŒé…ç½®ï¼Œç”Ÿæˆå®Œæ•´çš„æ¶ˆèå®éªŒç»“æœ
"""

import os
import sys
import time
import yaml
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import subprocess
import argparse
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ablation_experiments.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AblationExperimentRunner:
    """æ¶ˆèå®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_dir="config/ablation", results_dir="results/ablation", max_workers=4):
        self.config_dir = Path(config_dir)
        self.results_dir = Path(results_dir)
        self.max_workers = max_workers
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»“æœè®°å½•
        self.results_summary = []
        self.failed_configs = []
        
    def load_configs(self):
        """åŠ è½½æ‰€æœ‰é…ç½®æ–‡ä»¶"""
        config_files = list(self.config_dir.glob("*.yaml"))
        # æ’é™¤ç´¢å¼•æ–‡ä»¶
        config_files = [f for f in config_files if f.name != "config_index.yaml"]
        
        logger.info(f"æ‰¾åˆ° {len(config_files)} ä¸ªé…ç½®æ–‡ä»¶")
        return sorted(config_files)
    
    def run_single_experiment(self, config_file):
        """è¿è¡Œå•ä¸ªå®éªŒé…ç½®"""
        try:
            logger.info(f"å¼€å§‹è¿è¡Œå®éªŒ: {config_file.name}")
            
            # åŠ è½½é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # åˆ›å»ºå®éªŒç‰¹å®šçš„ç»“æœç›®å½•
            exp_name = config_file.stem
            exp_results_dir = self.results_dir / exp_name
            exp_results_dir.mkdir(exist_ok=True)
            
            # æ›´æ–°é…ç½®ä¸­çš„ä¿å­˜è·¯å¾„
            config['save_dir'] = str(exp_results_dir)
            config['save_options']['save_model'] = True
            config['save_options']['save_predictions'] = True
            config['save_options']['save_excel_results'] = True
            
            # ä¿å­˜æ›´æ–°çš„é…ç½®
            updated_config_file = exp_results_dir / "config.yaml"
            with open(updated_config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            # è¿è¡Œå®éªŒ
            start_time = time.time()
            
            # è°ƒç”¨ä¸»ç¨‹åº
            cmd = [sys.executable, "main.py", "--config", str(updated_config_file)]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.returncode == 0:
                logger.info(f"âœ… å®éªŒå®Œæˆ: {exp_name} (è€—æ—¶: {duration:.1f}ç§’)")
                
                # æ”¶é›†ç»“æœ
                result_info = self.collect_experiment_results(exp_name, exp_results_dir, duration)
                return result_info
            else:
                logger.error(f"âŒ å®éªŒå¤±è´¥: {exp_name}")
                logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return {
                    'config_name': exp_name,
                    'status': 'failed',
                    'error': result.stderr,
                    'duration': duration
                }
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° å®éªŒè¶…æ—¶: {exp_name}")
            return {
                'config_name': exp_name,
                'status': 'timeout',
                'error': 'Experiment timeout (1 hour)',
                'duration': 3600
            }
        except Exception as e:
            logger.error(f"ğŸ’¥ å®éªŒå¼‚å¸¸: {exp_name} - {str(e)}")
            return {
                'config_name': exp_name,
                'status': 'error',
                'error': str(e),
                'duration': 0
            }
    
    def collect_experiment_results(self, exp_name, exp_results_dir, duration):
        """æ”¶é›†å•ä¸ªå®éªŒçš„ç»“æœ"""
        result_info = {
            'config_name': exp_name,
            'status': 'completed',
            'duration': duration,
            'timestamp': datetime.now().isoformat()
        }
        
        # å°è¯•è¯»å–ç»“æœæ–‡ä»¶
        excel_file = exp_results_dir / "results.xlsx"
        if excel_file.exists():
            try:
                df = pd.read_excel(excel_file)
                if not df.empty:
                    # æå–å…³é”®æŒ‡æ ‡
                    result_info.update({
                        'mae': df.get('MAE', [np.nan])[0] if 'MAE' in df.columns else np.nan,
                        'rmse': df.get('RMSE', [np.nan])[0] if 'RMSE' in df.columns else np.nan,
                        'r2': df.get('RÂ²', [np.nan])[0] if 'RÂ²' in df.columns else np.nan,
                        'mape': df.get('MAPE', [np.nan])[0] if 'MAPE' in df.columns else np.nan
                    })
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶ {excel_file}: {e}")
        
        return result_info
    
    def run_parallel_experiments(self, config_files, max_workers=None):
        """å¹¶è¡Œè¿è¡Œå®éªŒ"""
        if max_workers is None:
            max_workers = self.max_workers
            
        logger.info(f"å¼€å§‹å¹¶è¡Œè¿è¡Œ {len(config_files)} ä¸ªå®éªŒ (æœ€å¤§å¹¶å‘: {max_workers})")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_config = {
                executor.submit(self.run_single_experiment, config_file): config_file
                for config_file in config_files
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_config):
                config_file = future_to_config[future]
                try:
                    result = future.result()
                    self.results_summary.append(result)
                    
                    if result['status'] != 'completed':
                        self.failed_configs.append(result)
                    
                    completed += 1
                    logger.info(f"è¿›åº¦: {completed}/{len(config_files)} ({completed/len(config_files)*100:.1f}%)")
                    
                except Exception as e:
                    logger.error(f"å®éªŒ {config_file.name} æ‰§è¡Œå¼‚å¸¸: {e}")
                    self.failed_configs.append({
                        'config_name': config_file.name,
                        'status': 'exception',
                        'error': str(e),
                        'duration': 0
                    })
        
        logger.info(f"æ‰€æœ‰å®éªŒå®Œæˆ! æˆåŠŸ: {len(self.results_summary)-len(self.failed_configs)}, å¤±è´¥: {len(self.failed_configs)}")
    
    def save_results_summary(self):
        """ä¿å­˜å®éªŒç»“æœæ‘˜è¦"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        summary_file = self.results_dir / "experiment_summary.csv"
        df_summary = pd.DataFrame(self.results_summary)
        df_summary.to_csv(summary_file, index=False, encoding='utf-8')
        logger.info(f"å®éªŒç»“æœæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        # ä¿å­˜å¤±è´¥é…ç½®
        if self.failed_configs:
            failed_file = self.results_dir / "failed_configs.csv"
            df_failed = pd.DataFrame(self.failed_configs)
            df_failed.to_csv(failed_file, index=False, encoding='utf-8')
            logger.info(f"å¤±è´¥é…ç½®å·²ä¿å­˜: {failed_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_statistics_report()
    
    def generate_statistics_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        report_file = self.results_dir / "statistics_report.md"
        
        df = pd.DataFrame(self.results_summary)
        completed_df = df[df['status'] == 'completed']
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Project1140 æ¶ˆèå®éªŒç»“æœç»Ÿè®¡æŠ¥å‘Š\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»é…ç½®æ•°**: {len(df)}\n")
            f.write(f"- **æˆåŠŸå®Œæˆ**: {len(completed_df)}\n")
            f.write(f"- **å¤±è´¥é…ç½®**: {len(df) - len(completed_df)}\n")
            f.write(f"- **æˆåŠŸç‡**: {len(completed_df)/len(df)*100:.1f}%\n\n")
            
            if len(completed_df) > 0:
                # æ€§èƒ½ç»Ÿè®¡
                f.write("## æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡\n\n")
                metrics = ['mae', 'rmse', 'r2', 'mape']
                for metric in metrics:
                    if metric in completed_df.columns:
                        values = completed_df[metric].dropna()
                        if len(values) > 0:
                            f.write(f"### {metric.upper()}\n")
                            f.write(f"- **å¹³å‡å€¼**: {values.mean():.4f}\n")
                            f.write(f"- **æ ‡å‡†å·®**: {values.std():.4f}\n")
                            f.write(f"- **æœ€å°å€¼**: {values.min():.4f}\n")
                            f.write(f"- **æœ€å¤§å€¼**: {values.max():.4f}\n\n")
                
                # æ¨¡å‹æ¯”è¾ƒ
                f.write("## æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ\n\n")
                if 'config_name' in completed_df.columns:
                    # æå–æ¨¡å‹åç§°
                    completed_df['model'] = completed_df['config_name'].str.split('_').str[0]
                    
                    for metric in metrics:
                        if metric in completed_df.columns:
                            model_stats = completed_df.groupby('model')[metric].agg(['mean', 'std', 'count']).round(4)
                            f.write(f"### {metric.upper()} æŒ‰æ¨¡å‹åˆ†ç»„\n\n")
                            f.write(model_stats.to_string())
                            f.write("\n\n")
            
            # å¤±è´¥åˆ†æ
            if len(self.failed_configs) > 0:
                f.write("## å¤±è´¥é…ç½®åˆ†æ\n\n")
                failed_df = pd.DataFrame(self.failed_configs)
                status_counts = failed_df['status'].value_counts()
                f.write("### å¤±è´¥ç±»å‹åˆ†å¸ƒ\n\n")
                for status, count in status_counts.items():
                    f.write(f"- **{status}**: {count}\n")
                f.write("\n")
        
        logger.info(f"ç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    parser = argparse.ArgumentParser(description="è¿è¡ŒProject1140æ¶ˆèå®éªŒ")
    parser.add_argument("--config-dir", default="config/ablation", help="é…ç½®æ–‡ä»¶ç›®å½•")
    parser.add_argument("--results-dir", default="results/ablation", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--max-workers", type=int, default=4, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--start-from", type=int, default=0, help="ä»ç¬¬å‡ ä¸ªé…ç½®å¼€å§‹è¿è¡Œ")
    parser.add_argument("--max-configs", type=int, help="æœ€å¤§è¿è¡Œé…ç½®æ•°")
    parser.add_argument("--model-filter", help="åªè¿è¡ŒæŒ‡å®šæ¨¡å‹ (å¦‚: Transformer,LSTM)")
    parser.add_argument("--dry-run", action="store_true", help="åªåˆ—å‡ºé…ç½®ï¼Œä¸å®é™…è¿è¡Œ")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = AblationExperimentRunner(
        config_dir=args.config_dir,
        results_dir=args.results_dir,
        max_workers=args.max_workers
    )
    
    # åŠ è½½é…ç½®
    config_files = runner.load_configs()
    
    # åº”ç”¨è¿‡æ»¤å™¨
    if args.model_filter:
        models = args.model_filter.split(',')
        config_files = [f for f in config_files if any(model in f.name for model in models)]
        logger.info(f"åº”ç”¨æ¨¡å‹è¿‡æ»¤å™¨åå‰©ä½™ {len(config_files)} ä¸ªé…ç½®")
    
    # åº”ç”¨èŒƒå›´é™åˆ¶
    if args.start_from > 0:
        config_files = config_files[args.start_from:]
        logger.info(f"ä»ç¬¬ {args.start_from} ä¸ªé…ç½®å¼€å§‹è¿è¡Œ")
    
    if args.max_configs:
        config_files = config_files[:args.max_configs]
        logger.info(f"é™åˆ¶è¿è¡Œ {args.max_configs} ä¸ªé…ç½®")
    
    if args.dry_run:
        logger.info("Dry runæ¨¡å¼ - åªåˆ—å‡ºé…ç½®:")
        for i, config_file in enumerate(config_files):
            logger.info(f"{i+1:3d}. {config_file.name}")
        return
    
    # è¿è¡Œå®éªŒ
    start_time = time.time()
    runner.run_parallel_experiments(config_files)
    end_time = time.time()
    
    # ä¿å­˜ç»“æœ
    runner.save_results_summary()
    
    total_duration = end_time - start_time
    logger.info(f"ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ! æ€»è€—æ—¶: {total_duration/3600:.1f} å°æ—¶")

if __name__ == "__main__":
    main()
