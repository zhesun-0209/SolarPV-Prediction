#!/usr/bin/env python3
"""
GPUä¼˜åŒ–å™¨ - é’ˆå¯¹A100ç­‰é«˜ç«¯GPUçš„ä¼˜åŒ–ç­–ç•¥
åŠ¨æ€è°ƒæ•´å¹¶è¡Œæ•°å’Œå†…å­˜ä½¿ç”¨
"""

import os
import sys
import time
import torch
import psutil
import subprocess
import logging
from pathlib import Path
import threading
import queue

logger = logging.getLogger(__name__)

class GPUOptimizer:
    """GPUä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        self.gpu_memory_total = []
        self.gpu_memory_used = []
        
        if self.gpu_count > 0:
            for i in range(self.gpu_count):
                props = torch.cuda.get_device_properties(i)
                total_memory = props.total_memory / 1024**3  # GB
                self.gpu_memory_total.append(total_memory)
                self.gpu_memory_used.append(0)
        
        logger.info(f"ğŸ¯ GPUä¼˜åŒ–å™¨åˆå§‹åŒ–: {self.gpu_count} ä¸ªGPU")
        for i, memory in enumerate(self.gpu_memory_total):
            logger.info(f"   GPU {i}: {memory:.1f}GB")
    
    def get_optimal_gpu_parallel_count(self, model_type='mixed'):
        """è·å–æœ€ä¼˜GPUå¹¶è¡Œæ•°"""
        if self.gpu_count == 0:
            return 0
        
        # è·å–å½“å‰GPUä½¿ç”¨æƒ…å†µ
        current_usage = self.get_gpu_memory_usage()
        
        # æ ¹æ®GPUç±»å‹å’Œå†…å­˜ä½¿ç”¨æƒ…å†µè®¡ç®—æœ€ä¼˜å¹¶è¡Œæ•°
        if self.gpu_memory_total[0] >= 80:  # A100 80GB
            base_parallel = 16
            memory_factor = 0.8
        elif self.gpu_memory_total[0] >= 40:  # A100 40GB æˆ–å…¶ä»–é«˜ç«¯GPU
            base_parallel = 12
            memory_factor = 0.7
        elif self.gpu_memory_total[0] >= 24:  # RTX 3090/4090ç­‰
            base_parallel = 8
            memory_factor = 0.6
        else:  # å…¶ä»–GPU
            base_parallel = 4
            memory_factor = 0.5
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´
        if model_type == 'transformer':
            model_factor = 0.8  # Transformeréœ€è¦æ›´å¤šå†…å­˜
        elif model_type == 'lstm':
            model_factor = 1.0  # LSTMå†…å­˜éœ€æ±‚ä¸­ç­‰
        elif model_type == 'tcn':
            model_factor = 1.2  # TCNå†…å­˜éœ€æ±‚è¾ƒå°
        else:  # mixed
            model_factor = 1.0
        
        # æ ¹æ®å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´
        if current_usage[0] > 0.8:  # å†…å­˜ä½¿ç”¨è¶…è¿‡80%
            memory_factor *= 0.5
        elif current_usage[0] > 0.6:  # å†…å­˜ä½¿ç”¨è¶…è¿‡60%
            memory_factor *= 0.7
        elif current_usage[0] > 0.4:  # å†…å­˜ä½¿ç”¨è¶…è¿‡40%
            memory_factor *= 0.85
        
        optimal_parallel = max(1, int(base_parallel * memory_factor * model_factor))
        
        logger.info(f"ğŸ¯ æœ€ä¼˜GPUå¹¶è¡Œæ•°: {optimal_parallel} (å†…å­˜ä½¿ç”¨: {current_usage[0]:.1%})")
        return optimal_parallel
    
    def get_gpu_memory_usage(self):
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.gpu_count == 0:
            return [0]
        
        usage = []
        for i in range(self.gpu_count):
            try:
                # ä½¿ç”¨nvidia-ml-pyæˆ–nvidia-smiè·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
                result = subprocess.run([
                    'nvidia-smi', '--query-gpu=memory.used,memory.total',
                    '--format=csv,noheader,nounits', f'--id={i}'
                ], capture_output=True, text=True)
                
                if result.returncode == 0:
                    used, total = map(int, result.stdout.strip().split(', '))
                    usage.append(used / total)
                else:
                    usage.append(0)
            except:
                usage.append(0)
        
        return usage
    
    def optimize_training_params(self, config, gpu_id=0):
        """ä¼˜åŒ–è®­ç»ƒå‚æ•°"""
        if self.gpu_count == 0:
            return config
        
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´æ‰¹å¤„ç†å¤§å°
        gpu_memory = self.gpu_memory_total[gpu_id]
        current_batch_size = config.get('train_params', {}).get('batch_size', 32)
        
        if gpu_memory >= 80:  # A100 80GB
            optimal_batch_size = min(128, current_batch_size * 4)
        elif gpu_memory >= 40:  # A100 40GB
            optimal_batch_size = min(96, current_batch_size * 3)
        elif gpu_memory >= 24:  # RTX 3090/4090
            optimal_batch_size = min(64, current_batch_size * 2)
        else:
            optimal_batch_size = current_batch_size
        
        # æ›´æ–°é…ç½®
        if 'train_params' not in config:
            config['train_params'] = {}
        
        config['train_params']['batch_size'] = optimal_batch_size
        
        # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        config['train_params']['use_amp'] = True
        
        # ä¼˜åŒ–å­¦ä¹ ç‡
        base_lr = config['train_params'].get('learning_rate', 5e-4)
        if optimal_batch_size > current_batch_size:
            # çº¿æ€§ç¼©æ”¾å­¦ä¹ ç‡
            config['train_params']['learning_rate'] = base_lr * (optimal_batch_size / current_batch_size)
        
        # è®¾ç½®GPUç‰¹å®šä¼˜åŒ–
        config['train_params']['gpu_optimizations'] = {
            'use_amp': True,
            'gradient_accumulation_steps': max(1, optimal_batch_size // 32),
            'dataloader_num_workers': min(8, os.cpu_count() // self.gpu_count),
            'pin_memory': True
        }
        
        logger.info(f"ğŸ¯ GPU {gpu_id} ä¼˜åŒ–: æ‰¹å¤„ç†å¤§å° {current_batch_size} -> {optimal_batch_size}")
        return config
    
    def monitor_gpu_usage(self, interval=30):
        """ç›‘æ§GPUä½¿ç”¨æƒ…å†µ"""
        def monitor_loop():
            while True:
                try:
                    usage = self.get_gpu_memory_usage()
                    for i, use in enumerate(usage):
                        if use > 0.9:  # å†…å­˜ä½¿ç”¨è¶…è¿‡90%
                            logger.warning(f"âš ï¸ GPU {i} å†…å­˜ä½¿ç”¨è¿‡é«˜: {use:.1%}")
                        elif use > 0.8:  # å†…å­˜ä½¿ç”¨è¶…è¿‡80%
                            logger.info(f"ğŸ“Š GPU {i} å†…å­˜ä½¿ç”¨: {use:.1%}")
                    
                    time.sleep(interval)
                except Exception as e:
                    logger.error(f"GPUç›‘æ§å¼‚å¸¸: {e}")
                    time.sleep(interval)
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        return monitor_thread
    
    def get_system_recommendations(self):
        """è·å–ç³»ç»Ÿä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # GPUå»ºè®®
        if self.gpu_count > 0:
            if self.gpu_memory_total[0] >= 80:
                recommendations.append("ğŸš€ A100 80GBæ£€æµ‹åˆ°ï¼Œå»ºè®®GPUå¹¶è¡Œæ•°: 16")
                recommendations.append("ğŸ’¡ å¯ä»¥å¯ç”¨å¤§æ‰¹æ¬¡è®­ç»ƒå’Œæ··åˆç²¾åº¦")
            elif self.gpu_memory_total[0] >= 40:
                recommendations.append("ğŸ¯ A100 40GBæ£€æµ‹åˆ°ï¼Œå»ºè®®GPUå¹¶è¡Œæ•°: 12")
                recommendations.append("ğŸ’¡ å¯ä»¥å¯ç”¨ä¸­ç­‰æ‰¹æ¬¡è®­ç»ƒ")
            else:
                recommendations.append(f"ğŸ’» GPUæ£€æµ‹åˆ°ï¼Œå»ºè®®GPUå¹¶è¡Œæ•°: {min(8, self.gpu_count * 2)}")
        
        # CPUå»ºè®®
        cpu_count = psutil.cpu_count()
        memory_gb = psutil.virtual_memory().total / 1024**3
        
        if memory_gb >= 128:
            recommendations.append(f"ğŸ’» å¤§å†…å­˜ç³»ç»Ÿï¼Œå»ºè®®CPUå¹¶è¡Œæ•°: {min(32, cpu_count * 2)}")
        elif memory_gb >= 64:
            recommendations.append(f"ğŸ’» ä¸­ç­‰å†…å­˜ç³»ç»Ÿï¼Œå»ºè®®CPUå¹¶è¡Œæ•°: {min(24, cpu_count)}")
        else:
            recommendations.append(f"ğŸ’» å°å†…å­˜ç³»ç»Ÿï¼Œå»ºè®®CPUå¹¶è¡Œæ•°: {min(16, cpu_count // 2)}")
        
        # ç¯å¢ƒå˜é‡å»ºè®®
        recommendations.append("ğŸ”§ å»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡:")
        recommendations.append("   export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128")
        recommendations.append("   export PYTORCH_CUDNN_V8_API_ENABLED=1")
        
        return recommendations

def test_gpu_optimizer():
    """æµ‹è¯•GPUä¼˜åŒ–å™¨"""
    optimizer = GPUOptimizer()
    
    print("ğŸ¯ GPUä¼˜åŒ–å™¨æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•å¹¶è¡Œæ•°è®¡ç®—
    optimal_parallel = optimizer.get_optimal_gpu_parallel_count()
    print(f"æœ€ä¼˜GPUå¹¶è¡Œæ•°: {optimal_parallel}")
    
    # æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ
    usage = optimizer.get_gpu_memory_usage()
    print(f"GPUå†…å­˜ä½¿ç”¨: {[f'{u:.1%}' for u in usage]}")
    
    # æµ‹è¯•è®­ç»ƒå‚æ•°ä¼˜åŒ–
    test_config = {
        'train_params': {
            'batch_size': 32,
            'learning_rate': 5e-4
        }
    }
    
    optimized_config = optimizer.optimize_training_params(test_config)
    print(f"ä¼˜åŒ–åæ‰¹å¤„ç†å¤§å°: {optimized_config['train_params']['batch_size']}")
    print(f"ä¼˜åŒ–åå­¦ä¹ ç‡: {optimized_config['train_params']['learning_rate']}")
    
    # è·å–ç³»ç»Ÿå»ºè®®
    recommendations = optimizer.get_system_recommendations()
    print("\nğŸ“‹ ç³»ç»Ÿä¼˜åŒ–å»ºè®®:")
    for rec in recommendations:
        print(f"   {rec}")
    
    # å¯åŠ¨ç›‘æ§
    print("\nğŸ“Š å¯åŠ¨GPUç›‘æ§...")
    monitor_thread = optimizer.monitor_gpu_usage(interval=5)
    
    try:
        time.sleep(20)  # ç›‘æ§20ç§’
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")

if __name__ == "__main__":
    test_gpu_optimizer()
