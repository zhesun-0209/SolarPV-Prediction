#!/usr/bin/env python3
"""
Colabè¿è¡Œè„šæœ¬
åœ¨Colabä¸­è¿è¡Œå®éªŒï¼Œç»“æœä¿å­˜åˆ°resultæ–‡ä»¶å¤¹
"""

import os
import subprocess
import sys
import time
import pandas as pd
import matplotlib.pyplot as plt

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒ...")
    
    # æ£€æŸ¥CUDA
    try:
        import torch
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"Device: {torch.cuda.get_device_name(0)}")
    except ImportError:
        print("PyTorch not available")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if os.path.exists('data/Project1033.csv'):
        print("âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨")
    else:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    return True

def setup_a100():
    """è®¾ç½®A100ä¼˜åŒ–"""
    try:
        import torch
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.cuda.set_per_process_memory_fraction(0.9)
        print("âœ… A100ä¼˜åŒ–è®¾ç½®å®Œæˆ")
    except ImportError:
        print("PyTorch not available, skipping A100 optimization")

def run_experiment(model, description):
    """è¿è¡Œå®éªŒ"""
    print(f"\nğŸš€ è¿è¡Œ {description}")
    print("-" * 50)
    
    cmd = [
        sys.executable, 'main.py',
        '--config', 'config/default.yaml',
        '--model', model,
        '--use_hist_weather', 'true',
        '--use_forecast', 'true',
        '--model_complexity', 'medium',
        '--past_days', '3'
    ]
    
    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True)
    end_time = time.time()
    
    if result.returncode == 0:
        print(f"âœ… {description} å®Œæˆ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
        return True
    else:
        print(f"âŒ {description} å¤±è´¥")
        print(f"é”™è¯¯: {result.stderr}")
        return False

def analyze_results():
    """åˆ†æç»“æœ"""
    print("\nğŸ“Š åˆ†æç»“æœ...")
    
    try:
        import glob
        
        # æŸ¥æ‰¾æ‰€æœ‰summary.csvæ–‡ä»¶
        summary_files = glob.glob('result/**/summary.csv', recursive=True)
        
        if not summary_files:
            print("âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(summary_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = []
        for file in summary_files:
            try:
                df = pd.read_csv(file)
                all_results.append(df)
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file}: {e}")
        
        if not all_results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœæ–‡ä»¶")
            return
        
        # åˆå¹¶ç»“æœ
        combined_df = pd.concat(all_results, ignore_index=True)
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        combined_df.to_csv('result/combined_results.csv', index=False)
        print("âœ… åˆå¹¶ç»“æœä¿å­˜åˆ° result/combined_results.csv")
        
        # æ˜¾ç¤ºç»“æœ
        print("\n=== æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")
        display_df = combined_df[['model', 'test_loss', 'rmse', 'mae', 'train_time_sec', 'inference_time_sec']].round(4)
        print(display_df)
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(15, 10))
        
        # RMSEå¯¹æ¯”
        plt.subplot(2, 3, 1)
        plt.bar(combined_df['model'], combined_df['rmse'])
        plt.title('RMSE Comparison')
        plt.xticks(rotation=45)
        plt.ylabel('RMSE')
        
        # MAEå¯¹æ¯”
        plt.subplot(2, 3, 2)
        plt.bar(combined_df['model'], combined_df['mae'])
        plt.title('MAE Comparison')
        plt.xticks(rotation=45)
        plt.ylabel('MAE')
        
        # è®­ç»ƒæ—¶é—´å¯¹æ¯”
        plt.subplot(2, 3, 3)
        plt.bar(combined_df['model'], combined_df['train_time_sec'])
        plt.title('Training Time Comparison')
        plt.xticks(rotation=45)
        plt.ylabel('Time (s)')
        
        # æ¨ç†æ—¶é—´å¯¹æ¯”
        plt.subplot(2, 3, 4)
        plt.bar(combined_df['model'], combined_df['inference_time_sec'])
        plt.title('Inference Time Comparison')
        plt.xticks(rotation=45)
        plt.ylabel('Time (s)')
        
        # å‚æ•°æ•°é‡å¯¹æ¯”
        plt.subplot(2, 3, 5)
        plt.bar(combined_df['model'], combined_df['param_count'])
        plt.title('Parameter Count Comparison')
        plt.xticks(rotation=45)
        plt.ylabel('Parameters')
        
        # æ€§èƒ½vsæ—¶é—´æ•£ç‚¹å›¾
        plt.subplot(2, 3, 6)
        plt.scatter(combined_df['train_time_sec'], combined_df['rmse'], s=100)
        for i, model in enumerate(combined_df['model']):
            plt.annotate(model, (combined_df['train_time_sec'].iloc[i], combined_df['rmse'].iloc[i]))
        plt.xlabel('Training Time (s)')
        plt.ylabel('RMSE')
        plt.title('Performance vs Training Time')
        
        plt.tight_layout()
        plt.savefig('result/performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = combined_df.loc[combined_df['rmse'].idxmin()]
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
        print(f"   RMSE: {best_model['rmse']:.4f}")
        print(f"   MAE: {best_model['mae']:.4f}")
        print(f"   è®­ç»ƒæ—¶é—´: {best_model['train_time_sec']:.2f}ç§’")
        print(f"   æ¨ç†æ—¶é—´: {best_model['inference_time_sec']:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ åˆ†æç»“æœæ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ SolarPV-Prediction Colabè¿è¡Œå™¨")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        return
    
    # è®¾ç½®A100ä¼˜åŒ–
    setup_a100()
    
    # æ£€æŸ¥Driveç›®å½•å¹¶ä¿®æ”¹é…ç½®
    drive_dir = '/content/drive/MyDrive/Solar PV electricity/results'
    if os.path.exists(drive_dir):
        print(f"âœ… Driveç›®å½•å­˜åœ¨: {drive_dir}")
        # ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼Œä¿å­˜åˆ°Driveç›®å½•
        import yaml
        with open('config/default.yaml', 'r') as f:
            config = yaml.safe_load(f)
        config['save_dir'] = drive_dir
        with open('config/default.yaml', 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"âœ… é…ç½®å·²æ›´æ–°ï¼Œç»“æœå°†ä¿å­˜åˆ°: {drive_dir}")
    else:
        print(f"âš ï¸ Driveç›®å½•ä¸å­˜åœ¨: {drive_dir}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²æŒ‚è½½Google Drive")
        # åˆ›å»ºæœ¬åœ°ç»“æœç›®å½•
        if not os.path.exists('result'):
            os.makedirs('result')
            print("âœ… åˆ›å»ºæœ¬åœ°resultç›®å½•")
    
    # è¿è¡Œå®éªŒ
    models = [
        ('TCN', 'TCNæ¨¡å‹'),
        ('LSTM', 'LSTMæ¨¡å‹'),
        ('Transformer', 'Transformeræ¨¡å‹'),
        ('XGB', 'XGBoostæ¨¡å‹')
    ]
    
    success_count = 0
    for model, description in models:
        if run_experiment(model, description):
            success_count += 1
    
    # åˆ†æç»“æœ
    analyze_results()
    
    print(f"\nğŸ‰ å®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸ: {success_count}/{len(models)} ä¸ªæ¨¡å‹")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: result/ ç›®å½•")
    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾: result/performance_comparison.png")

if __name__ == "__main__":
    main()
