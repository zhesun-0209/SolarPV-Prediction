#!/usr/bin/env python3
"""
æ•æ„Ÿæ€§åˆ†æå®éªŒè¿è¡Œè„šæœ¬
åœ¨å‰20ä¸ªå‚ä¸Šè¿è¡Œæ•æ„Ÿæ€§åˆ†æå®éªŒï¼Œç»“æœä¿å­˜åˆ°Google Drive
"""

import os
import sys
import time
import subprocess
import yaml
import pandas as pd
from pathlib import Path
import re
import uuid

def check_drive_mount():
    """æ£€æŸ¥Google Driveæ˜¯å¦å·²æŒ‚è½½"""
    drive_path = "/content/drive/MyDrive"
    if os.path.exists(drive_path):
        print("âœ… Google Driveå·²æŒ‚è½½")
        return True
    else:
        print("âŒ Google DriveæœªæŒ‚è½½ï¼Œè¯·å…ˆæŒ‚è½½Drive")
        return False

def get_data_files():
    """æ‰«ædataç›®å½•ï¼Œè·å–å‰20ä¸ªé¡¹ç›®çš„CSVæ–‡ä»¶"""
    data_dir = "data"
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    csv_files = []
    for file in os.listdir(data_dir):
        if file.startswith("Project") and file.endswith(".csv"):
            project_id = file.replace("Project", "").replace(".csv", "")
            csv_files.append((project_id, os.path.join(data_dir, file)))
    
    # æŒ‰é¡¹ç›®IDæ’åºï¼Œå–å‰20ä¸ª
    csv_files.sort(key=lambda x: int(x[0]))
    return csv_files[:20]

def get_sensitivity_config_files():
    """è·å–æ‰€æœ‰æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶"""
    config_dir = "sensitivity_analysis/configs"
    all_config_files = []
    
    # éœ€è¦è·³è¿‡çš„éå®éªŒé…ç½®æ–‡ä»¶
    skip_files = {
        'sensitivity_index.yaml',
        'sensitivity_global_index.yaml', 
        'index.yaml',
        'README.yaml',
        'readme.yaml'
    }
    
    if os.path.exists(config_dir):
        for project_dir in os.listdir(config_dir):
            project_path = os.path.join(config_dir, project_dir)
            if os.path.isdir(project_path):
                for config_file in os.listdir(project_path):
                    if config_file.endswith('.yaml') and config_file not in skip_files:
                        all_config_files.append(os.path.join(project_path, config_file))
    
    return all_config_files

def get_completed_experiments_for_project(project_id, drive_path):
    """è·å–æŒ‡å®šé¡¹ç›®å·²å®Œæˆçš„æ•æ„Ÿæ€§åˆ†æå®éªŒåˆ—è¡¨"""
    completed_configs = set()
    
    try:
        project_csv = os.path.join(drive_path, f"{project_id}_sensitivity_results.csv")
        if os.path.exists(project_csv):
            df = pd.read_csv(project_csv)
            if 'config_file' in df.columns:
                completed_configs = set(df['config_file'].tolist())
                print(f"ğŸ“Š é¡¹ç›® {project_id} å·²å®Œæˆæ•æ„Ÿæ€§åˆ†æå®éªŒ: {len(completed_configs)} ä¸ª")
            else:
                print(f"âš ï¸ é¡¹ç›® {project_id} CSVæ–‡ä»¶ç¼ºå°‘config_fileåˆ—")
        else:
            print(f"ğŸ“„ é¡¹ç›® {project_id} æ•æ„Ÿæ€§åˆ†æç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–é¡¹ç›® {project_id} æ•æ„Ÿæ€§åˆ†æç»“æœæ–‡ä»¶: {e}")
    
    return completed_configs

def run_experiment(config_file, data_file, project_id):
    """è¿è¡Œå•ä¸ªæ•æ„Ÿæ€§åˆ†æå®éªŒ"""
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„data_path
        config['data_path'] = data_file
        config['plant_id'] = project_id
        
        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨UUIDé¿å…å†²çªï¼‰
        temp_config = f"temp_sensitivity_config_{project_id}_{uuid.uuid4().hex[:8]}.yaml"
        with open(temp_config, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        # è¿è¡Œå®éªŒå¹¶è®°å½•æ—¶é—´
        cmd = ['python', 'main.py', '--config', temp_config]
        start_time = time.time()
        
        # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60åˆ†é’Ÿ
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
        duration = time.time() - start_time
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_config):
            os.remove(temp_config)
        
        if result.returncode == 0:
            return True, result.stdout, result.stderr, duration, config
        else:
            return False, result.stdout, result.stderr, duration, config
            
    except subprocess.TimeoutExpired:
        # å¤„ç†è¶…æ—¶
        if os.path.exists(temp_config):
            os.remove(temp_config)
        return False, "", "å®éªŒè¶…æ—¶ï¼ˆ60åˆ†é’Ÿï¼‰", 3600.0, {}
    except Exception as e:
        return False, "", str(e), 0.0, {}

def parse_experiment_output(output, config_file, duration, config):
    """è§£ææ•æ„Ÿæ€§åˆ†æå®éªŒè¾“å‡ºï¼Œæå–ç»“æœ"""
    try:
        # æå–æ¨¡å‹ä¿¡æ¯
        model_name = config.get('model', 'Unknown')
        complexity = config.get('model_complexity', 'unknown')
        
        # æå–å®éªŒå‚æ•°
        weather_level = config.get('weather_category', 'unknown')
        lookback_hours = config.get('past_hours', 24)
        complexity_level = complexity.replace('level', '') if 'level' in complexity else 'unknown'
        dataset_scale = 'Unknown'
        
        # æ ¹æ®train_ratioç¡®å®šdataset_scale
        train_ratio = config.get('train_ratio', 0.8)
        if train_ratio == 0.5:
            dataset_scale = 'Low'
        elif train_ratio == 0.67:
            dataset_scale = 'Medium'
        elif train_ratio == 0.75:
            dataset_scale = 'High'
        elif train_ratio == 0.8:
            dataset_scale = 'Full'
        
        # æå–è®­ç»ƒå‚æ•°
        use_pv = config.get('use_pv', False)
        use_hist_weather = config.get('use_hist_weather', False)
        use_forecast = config.get('use_forecast', False)
        time_encoding = config.get('use_time_encoding', False)
        past_days = config.get('past_days', 1)
        use_ideal_nwp = config.get('use_ideal_nwp', False)
        selected_features = config.get('selected_weather_features', [])
        
        # æå–æŒ‡æ ‡
        mse_match = re.search(r'mse=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        rmse_match = re.search(r'rmse=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        mae_match = re.search(r'mae=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        r_square_match = re.search(r'r_square=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        
        # è®¡ç®—æŒ‡æ ‡
        mse = float(mse_match.group(1)) if mse_match else 0.0
        rmse = float(rmse_match.group(1)) if rmse_match else 0.0
        mae = float(mae_match.group(1)) if mae_match else 0.0
        r_square = float(r_square_match.group(1)) if r_square_match else 0.0
        
        # åˆå§‹åŒ–é¢å¤–å­—æ®µ
        inference_time = 0.0
        param_count = 0
        samples_count = 0
        best_epoch = 0
        final_lr = 0.0
        nrmse = 0.0
        smape = 0.0
        gpu_memory_used = 0
        
        # ä½¿ç”¨METRICSæ ‡ç­¾æå–é¢å¤–ä¿¡æ¯
        for line in output.split('\n'):
            if "[METRICS]" in line:
                metrics_in_line = re.findall(r'(\w+)=([0-9.-]+)', line)
                for key, value_str in metrics_in_line:
                    try:
                        if key == 'inference_time':
                            inference_time = float(value_str)
                        elif key == 'param_count':
                            param_count = int(float(value_str))
                        elif key == 'samples_count':
                            samples_count = int(float(value_str))
                        elif key == 'best_epoch':
                            if value_str.lower() == 'nan':
                                best_epoch = 0
                            else:
                                best_epoch = int(float(value_str))
                        elif key == 'final_lr':
                            if value_str.lower() == 'nan':
                                final_lr = 0.0
                            else:
                                final_lr = float(value_str)
                        elif key == 'nrmse':
                            nrmse = float(value_str)
                        elif key == 'smape':
                            smape = float(value_str)
                        elif key == 'gpu_memory_used':
                            gpu_memory_used = int(float(value_str))
                    except Exception as e:
                        print(f"ğŸ” è°ƒè¯•ï¼š{key}æå–å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ä»METRICSä¸­æå–åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
        if inference_time == 0.0:
            inference_match = re.search(r'Inference time: ([\d.]+)s', output)
            inference_time = float(inference_match.group(1)) if inference_match else 0.0
        
        if param_count == 0:
            param_match = re.search(r'Total parameters: ([\d,]+)', output)
            param_count = int(param_match.group(1).replace(',', '')) if param_match else 0
        
        if samples_count == 0:
            samples_match = re.search(r'Training samples: (\d+)', output)
            samples_count = int(samples_match.group(1)) if samples_match else 0
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹
        is_dl_model = model_name in ['LSTM', 'GRU', 'Transformer', 'TCN']
        has_learning_rate = is_dl_model or model_name in ['XGB', 'LGBM']
        
        if best_epoch == 0 and is_dl_model:
            epoch_match = re.search(r'Best epoch: (\d+)', output)
            best_epoch = int(epoch_match.group(1)) if epoch_match else 0
        
        if final_lr == 0.0 and is_dl_model:
            lr_match = re.search(r'Final LR: ([\d.]+)', output)
            final_lr = float(lr_match.group(1)) if lr_match else 0.0
        
        if gpu_memory_used == 0:
            gpu_memory_match = re.search(r'GPU memory used: ([\d.]+)MB', output)
            gpu_memory_used = int(float(gpu_memory_match.group(1))) if gpu_memory_match else 0
        
        # è®¡ç®—NRMSEå’ŒSMAPEï¼ˆå¦‚æœæœªä»METRICSä¸­æå–ï¼‰
        if nrmse == 0.0 and mae > 0:
            nrmse = (rmse / (mae + 1e-8)) * 100
        
        if smape == 0.0 and mae > 0:
            smape = (2 * mae / (mae + 1e-8)) * 100
        
        # åˆ›å»ºç»“æœè¡Œ
        result_row = {
            'model': model_name,
            'weather_level': weather_level,
            'lookback_hours': lookback_hours,
            'complexity_level': complexity_level,
            'dataset_scale': dataset_scale,
            'use_pv': use_pv,
            'use_hist_weather': use_hist_weather,
            'use_forecast': use_forecast,
            'use_time_encoding': time_encoding,
            'past_days': past_days,
            'use_ideal_nwp': use_ideal_nwp,
            'selected_weather_features': str(selected_features),
            'epochs': config.get('epochs', 80 if complexity_level == '4' else 50) if is_dl_model else 0,
            'batch_size': config.get('train_params', {}).get('batch_size', 64) if is_dl_model else 0,
            'learning_rate': config.get('train_params', {}).get('learning_rate', 0.001) if has_learning_rate else 0.0,
            'train_time_sec': round(duration, 4),
            'inference_time_sec': inference_time,
            'param_count': param_count,
            'samples_count': samples_count,
            'best_epoch': best_epoch if is_dl_model else 0,
            'final_lr': final_lr if is_dl_model else 0.0,
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'nrmse': nrmse,
            'r_square': r_square,
            'smape': smape,
            'gpu_memory_used': gpu_memory_used,
            'config_file': os.path.basename(config_file)
        }
        
        return result_row
        
    except Exception as e:
        print(f"âš ï¸ è§£ææ•æ„Ÿæ€§åˆ†æå®éªŒç»“æœå¤±è´¥: {e}")
        return None

def create_project_csv(project_id, drive_path):
    """ä¸ºé¡¹ç›®åˆ›å»ºæ•æ„Ÿæ€§åˆ†æCSVæ–‡ä»¶"""
    csv_file = os.path.join(drive_path, f"{project_id}_sensitivity_results.csv")
    
    if not os.path.exists(csv_file):
        # åˆ›å»ºCSVæ–‡ä»¶å¤´
        columns = [
            'model', 'weather_level', 'lookback_hours', 'complexity_level', 'dataset_scale',
            'use_pv', 'use_hist_weather', 'use_forecast', 'use_time_encoding', 'past_days',
            'use_ideal_nwp', 'selected_weather_features', 'epochs', 'batch_size', 'learning_rate',
            'train_time_sec', 'inference_time_sec', 'param_count', 'samples_count', 'best_epoch',
            'final_lr', 'mse', 'rmse', 'mae', 'nrmse', 'r_square', 'smape', 'gpu_memory_used', 'config_file'
        ]
        
        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_file, index=False)
        print(f"ğŸ“„ åˆ›å»ºé¡¹ç›®æ•æ„Ÿæ€§åˆ†æCSVæ–‡ä»¶: {csv_file}")
        return True
    else:
        print(f"ğŸ“„ é¡¹ç›®æ•æ„Ÿæ€§åˆ†æCSVæ–‡ä»¶å·²å­˜åœ¨: {csv_file}")
        return True

def save_single_result_to_csv(result_row, project_id, drive_path):
    """ä¿å­˜å•ä¸ªæ•æ„Ÿæ€§åˆ†æç»“æœåˆ°é¡¹ç›®CSVæ–‡ä»¶"""
    try:
        csv_file = os.path.join(drive_path, f"{project_id}_sensitivity_results.csv")
        
        # è¯»å–ç°æœ‰CSV
        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
        else:
            df = pd.DataFrame()
        
        # æ·»åŠ æ–°è¡Œ
        new_row_df = pd.DataFrame([result_row])
        df = pd.concat([df, new_row_df], ignore_index=True)
        
        # ä¿å­˜CSV
        df.to_csv(csv_file, index=False)
        print(f"ğŸ’¾ æ•æ„Ÿæ€§åˆ†æç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
        print(f"ğŸ“Š CSVæ–‡ä»¶å½“å‰è¡Œæ•°: {len(df)}")
        print(f"ğŸ“Š æœ€æ–°å®éªŒ: {result_row['model']} - {result_row['mse']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•æ„Ÿæ€§åˆ†æç»“æœå¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è°ƒè¯•: è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ SolarPVé¡¹ç›® - æ•æ„Ÿæ€§åˆ†æå®éªŒè„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥Google Drive
    if not check_drive_mount():
        return
    
    # è®¾ç½®è·¯å¾„
    drive_path = "/content/drive/MyDrive/Solar PV electricity/sensitivity analysis"
    os.makedirs(drive_path, exist_ok=True)
    
    # è·å–æ•°æ®æ–‡ä»¶
    print("ğŸ“ æ‰«ææ•°æ®æ–‡ä»¶...")
    data_files = get_data_files()
    if not data_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼ˆå‰20ä¸ªå‚ï¼‰")
    
    # è·å–æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶
    print("ğŸ“ æ‰«ææ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶...")
    config_files = get_sensitivity_config_files()
    print(f"ğŸ“Š æ‰¾åˆ° {len(config_files)} ä¸ªæ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆé…ç½®æ–‡ä»¶
    if len(config_files) < len(data_files) * 100:
        print("ğŸ”§ æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶ä¸è¶³ï¼Œæ­£åœ¨ç”Ÿæˆ...")
        try:
            result = subprocess.run([
                'python', 'sensitivity_analysis/scripts/generate_sensitivity_configs.py'
            ], capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print("âœ… æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                config_files = get_sensitivity_config_files()
                print(f"ğŸ“Š ç°åœ¨æœ‰ {len(config_files)} ä¸ªæ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶")
            else:
                print(f"âŒ æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return
        except Exception as e:
            print(f"âŒ æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶ç”Ÿæˆå¼‚å¸¸: {e}")
            return
    
    # å¼€å§‹å®éªŒ
    all_results = []
    total_experiments = 0
    completed_experiments = 0
    failed_experiments = 0
    
    for project_idx, (project_id, data_file) in enumerate(data_files, 1):
        print(f"\nğŸš€ å¼€å§‹é¡¹ç›® {project_id} çš„æ•æ„Ÿæ€§åˆ†æå®éªŒ ({project_idx}/{len(data_files)})")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")
        
        # è·å–è¯¥é¡¹ç›®çš„æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶
        project_configs = [cf for cf in config_files if f"/{project_id}/" in cf]
        print(f"ğŸ“Š æ‰¾åˆ° {len(project_configs)} ä¸ªæ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶")
        
        if not project_configs:
            print(f"âš ï¸ é¡¹ç›® {project_id} æ²¡æœ‰æ•æ„Ÿæ€§åˆ†æé…ç½®æ–‡ä»¶ï¼Œè·³è¿‡")
            continue
        
        # åˆ›å»ºé¡¹ç›®CSVæ–‡ä»¶
        if not create_project_csv(project_id, drive_path):
            print(f"âŒ æ— æ³•ä¸ºé¡¹ç›® {project_id} åˆ›å»ºæ•æ„Ÿæ€§åˆ†æCSVæ–‡ä»¶")
            continue
        
        # è·å–è¯¥é¡¹ç›®å·²å®Œæˆçš„æ•æ„Ÿæ€§åˆ†æå®éªŒ
        completed_configs = get_completed_experiments_for_project(project_id, drive_path)
        
        project_results = []
        skipped_count = 0
        
        for config_file in project_configs:
            config_name = os.path.basename(config_file)
            total_experiments += 1
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if config_name in completed_configs:
                skipped_count += 1
                if skipped_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè·³è¿‡çš„å®éªŒ
                    print(f"â­ï¸ è·³è¿‡å·²å®Œæˆå®éªŒ: {config_name}")
                continue
            
            print(f"\nğŸ”„ è¿è¡Œæ•æ„Ÿæ€§åˆ†æå®éªŒ: {config_name}")
            
            # è¿è¡Œå®éªŒ
            success, stdout, stderr, duration, config = run_experiment(config_file, data_file, project_id)
            
            if success:
                # è§£æç»“æœ
                print(f"ğŸ” è°ƒè¯•: å¼€å§‹è§£ææ•æ„Ÿæ€§åˆ†æå®éªŒç»“æœ: {config_name}")
                result_row = parse_experiment_output(stdout, config_file, duration, config)
                if result_row:
                    project_results.append(result_row)
                    completed_experiments += 1
                    print(f"âœ… æ•æ„Ÿæ€§åˆ†æå®éªŒå®Œæˆ: {config_name} ({duration:.1f}s) - MSE: {result_row['mse']:.4f}")
                    print(f"ğŸ” è°ƒè¯•: è§£ææˆåŠŸï¼Œç»“æœå­—æ®µ: {list(result_row.keys())}")
                    print(f"ğŸ” è°ƒè¯•: å½“å‰project_resultsæ•°é‡: {len(project_results)}")
                    
                    # ç«‹å³ä¿å­˜åˆ°é¡¹ç›®CSV
                    print(f"ğŸ’¾ ç«‹å³ä¿å­˜æ•æ„Ÿæ€§åˆ†æç»“æœåˆ°é¡¹ç›®CSV...")
                    save_single_result_to_csv(result_row, project_id, drive_path)
                    print(f"âœ… æ•æ„Ÿæ€§åˆ†æç»“æœå·²ä¿å­˜åˆ°é¡¹ç›®CSV")
                else:
                    failed_experiments += 1
                    print(f"âš ï¸ æ— æ³•è§£ææ•æ„Ÿæ€§åˆ†æå®éªŒç»“æœ: {config_name}")
                    print(f"ğŸ” è°ƒè¯•: å®éªŒè¾“å‡ºå‰500å­—ç¬¦: {stdout[:500]}")
            else:
                failed_experiments += 1
                print(f"âŒ æ•æ„Ÿæ€§åˆ†æå®éªŒå¤±è´¥: {config_name}")
                print(f"   é”™è¯¯: {stderr}")
                print(f"ğŸ” è°ƒè¯•: æ ‡å‡†è¾“å‡º: {stdout[:200]}")
        
        if skipped_count > 5:
            print(f"â­ï¸ ... è¿˜æœ‰ {skipped_count - 5} ä¸ªå·²å®Œæˆçš„æ•æ„Ÿæ€§åˆ†æå®éªŒè¢«è·³è¿‡")
        
        # é¡¹ç›®å®Œæˆç»Ÿè®¡
        print(f"âœ… é¡¹ç›® {project_id} æ•æ„Ÿæ€§åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š é¡¹ç›® {project_id} æ•æ„Ÿæ€§åˆ†æç»Ÿè®¡:")
        print(f"   æ€»å®éªŒ: {len(project_configs)}")
        print(f"   è·³è¿‡: {skipped_count}")
        print(f"   å®Œæˆ: {len(project_results)}")
        print(f"   å¤±è´¥: {len(project_configs) - skipped_count - len(project_results)}")
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ•æ„Ÿæ€§åˆ†æå®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æ€»å®éªŒæ•°: {total_experiments}")
    print(f"âœ… å®Œæˆ: {completed_experiments}")
    print(f"âŒ å¤±è´¥: {failed_experiments}")
    print(f"â­ï¸ è·³è¿‡: {total_experiments - completed_experiments - failed_experiments}")
    
    if all_results:
        print(f"ğŸ’¾ æ€»å…±ä¿å­˜äº† {len(all_results)} ä¸ªæ•æ„Ÿæ€§åˆ†æç»“æœ")
        print(f"ğŸ“ æ•æ„Ÿæ€§åˆ†æç»“æœä¿å­˜åœ¨: {os.path.join(drive_path, 'SolarPV_Sensitivity_Analysis')}")

if __name__ == "__main__":
    main()
