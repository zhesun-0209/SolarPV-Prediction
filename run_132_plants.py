#!/usr/bin/env python3
"""
è¿è¡Œ132ä¸ªå‚çš„å®Œæ•´å®éªŒ
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦è·Ÿè¸ªå’ŒGPUåŠ é€Ÿ
"""

import os
import sys
import time
import subprocess
import pandas as pd
import glob
import argparse
from datetime import datetime

def find_plant_data_files():
    """æŸ¥æ‰¾æ‰€æœ‰å‚çš„æ•°æ®æ–‡ä»¶"""
    
    data_dir = 'data'
    plant_files = []
    
    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))
    
    for file in csv_files:
        filename = os.path.basename(file)
        # åŒ…å«æ‰€æœ‰CSVæ–‡ä»¶ä½œä¸ºå‚æ•°æ®æ–‡ä»¶
        if filename.endswith('.csv'):
            plant_id = filename.replace('.csv', '')
            plant_files.append((plant_id, file))
    
    return sorted(plant_files)

def check_existing_results(plant_id):
    """æ£€æŸ¥å‚æ˜¯å¦å·²æœ‰ç»“æœ"""
    
    # æ£€æŸ¥Driveå’Œæœ¬åœ°ç»“æœ
    drive_dir = '/content/drive/MyDrive/Solar PV electricity/results'
    local_dir = 'result'
    
    result_dirs = []
    if os.path.exists(drive_dir):
        result_dirs.append(drive_dir)
    if os.path.exists(local_dir):
        result_dirs.append(local_dir)
    
    for result_dir in result_dirs:
        plant_result_dir = os.path.join(result_dir, plant_id)
        if os.path.exists(plant_result_dir):
            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„ç»“æœ
            summary_files = glob.glob(os.path.join(plant_result_dir, '**/summary.csv'), recursive=True)
            if len(summary_files) >= 36:  # è‡³å°‘36ä¸ªå®éªŒï¼ˆ3ä¸ªæ¨¡å‹ Ã— 4ä¸ªç‰¹å¾ç»„åˆ Ã— 3ä¸ªå¤æ‚åº¦ï¼‰
                return True, plant_result_dir
    
    return False, None

def check_partial_results(plant_id):
    """æ£€æŸ¥å‚æ˜¯å¦æœ‰éƒ¨åˆ†ç»“æœï¼Œè¿”å›ç¼ºå¤±çš„å®éªŒ"""
    
    # æ£€æŸ¥Driveå’Œæœ¬åœ°ç»“æœ
    drive_dir = '/content/drive/MyDrive/Solar PV electricity/results'
    local_dir = 'result'
    
    result_dirs = []
    if os.path.exists(drive_dir):
        result_dirs.append(drive_dir)
    if os.path.exists(local_dir):
        result_dirs.append(local_dir)
    
    # å®šä¹‰æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„å®éªŒ
    models = ['Transformer', 'LSTM', 'GRU', 'TCN', 'RF', 'XGB', 'LGBM']
    feature_configs = [
        (False, False),  # æ— ç‰¹å¾
        (True, False),   # å†å²å¤©æ°”
        (False, True),   # é¢„æµ‹å¤©æ°”
        (True, True)     # å†å²+é¢„æµ‹å¤©æ°”
    ]
    complexities = ['low', 'medium', 'high']
    past_days_options = [1, 3, 7]
    
    expected_experiments = []
    for model in models:
        for hist_weather, forecast in feature_configs:
            for complexity in complexities:
                for past_days in past_days_options:
                    # ç”Ÿæˆå®éªŒID
                    feat_str = f"feat{str(hist_weather).lower()}_fcst{str(forecast).lower()}_days{past_days}_comp{complexity}"
                    expected_experiments.append(f"{model}_{feat_str}")
    
    # æŸ¥æ‰¾ç°æœ‰ç»“æœ
    existing_experiments = set()
    for result_dir in result_dirs:
        plant_result_dir = os.path.join(result_dir, plant_id)
        if os.path.exists(plant_result_dir):
            # æŸ¥æ‰¾æ‰€æœ‰summary.csvæ–‡ä»¶
            summary_files = glob.glob(os.path.join(plant_result_dir, '**/summary.csv'), recursive=True)
            for file in summary_files:
                # ä»æ–‡ä»¶è·¯å¾„æå–å®éªŒID
                path_parts = file.split(os.sep)
                if len(path_parts) >= 2:
                    exp_id = path_parts[-2]  # å‡è®¾å®éªŒIDæ˜¯ç›®å½•å
                    existing_experiments.add(exp_id)
    
    # æ‰¾å‡ºç¼ºå¤±çš„å®éªŒ
    missing_experiments = set(expected_experiments) - existing_experiments
    
    return len(missing_experiments) == 0, missing_experiments, len(existing_experiments)

def run_plant_experiments(plant_id, data_file, force_rerun=False):
    """è¿è¡Œå•ä¸ªå‚çš„æ‰€æœ‰å®éªŒ"""
    
    print(f"\nğŸ­ å¼€å§‹å¤„ç†å‚: {plant_id}")
    print(f"   æ•°æ®æ–‡ä»¶: {data_file}")
    print("=" * 80)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´ç»“æœ
    has_complete_results, result_dir = check_existing_results(plant_id)
    if has_complete_results and not force_rerun:
        print(f"âœ… å‚ {plant_id} å·²æœ‰å®Œæ•´ç»“æœï¼Œè·³è¿‡")
        return True
    
    # æ£€æŸ¥éƒ¨åˆ†ç»“æœ
    is_complete, missing_experiments, existing_count = check_partial_results(plant_id)
    
    if is_complete and not force_rerun:
        print(f"âœ… å‚ {plant_id} æ‰€æœ‰å®éªŒå·²å®Œæˆï¼Œè·³è¿‡")
        return True
    elif existing_count > 0:
        print(f"ğŸ“Š å‚ {plant_id} å·²æœ‰ {existing_count} ä¸ªå®éªŒï¼Œç¼ºå¤± {len(missing_experiments)} ä¸ª")
        print(f"   ç¼ºå¤±å®éªŒ: {list(missing_experiments)[:5]}{'...' if len(missing_experiments) > 5 else ''}")
    
    # è¿è¡Œå®éªŒ
    cmd = [
        sys.executable, 'colab_gpu_experiments.py',
        '--plant_id', plant_id,
        '--data_file', data_file
    ]
    
    print(f"ğŸš€ è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    start_time = time.time()
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… å‚ {plant_id} å®éªŒå®Œæˆ (è€—æ—¶: {duration:.1f}ç§’)")
            return True
        else:
            print(f"âŒ å‚ {plant_id} å®éªŒå¤±è´¥")
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print(f"âŒ å‚ {plant_id} å®éªŒè¶…æ—¶ (1å°æ—¶)")
        return False
    except Exception as e:
        print(f"âŒ å‚ {plant_id} å®éªŒå¼‚å¸¸: {e}")
        return False

def run_all_plants(force_rerun=False):
    """è¿è¡Œæ‰€æœ‰å‚çš„å®éªŒ"""
    
    print("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰132ä¸ªå‚çš„å®Œæ•´å®éªŒ")
    if force_rerun:
        print("âš ï¸  å¼ºåˆ¶é‡æ–°è¿è¡Œæ¨¡å¼")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰å‚æ•°æ®æ–‡ä»¶
    plant_files = find_plant_data_files()
    
    if not plant_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å‚æ•°æ®æ–‡ä»¶")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨ data/ ç›®å½•ä¸‹")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(plant_files)} ä¸ªå‚æ•°æ®æ–‡ä»¶")
    
    # è®¡ç®—æ€»å®éªŒæ•°
    models = ['Transformer', 'LSTM', 'GRU', 'TCN', 'RF', 'XGB', 'LGBM']
    feature_configs = 4  # æ— ç‰¹å¾, å†å²å¤©æ°”, é¢„æµ‹å¤©æ°”, å†å²+é¢„æµ‹å¤©æ°”
    complexities = 3     # low, medium, high
    past_days_options = 3  # 1, 3, 7å¤©
    experiments_per_plant = len(models) * feature_configs * complexities * past_days_options
    total_experiments = len(plant_files) * experiments_per_plant
    
    print(f"ğŸ“Š å®éªŒè§„æ¨¡:")
    print(f"   æ¯å‚å®éªŒæ•°: {experiments_per_plant}")
    print(f"   æ€»å®éªŒæ•°: {total_experiments:,}")
    print(f"   é¢„è®¡æ—¶é—´: {total_experiments * 2 / 60:.1f} å°æ—¶ (å‡è®¾æ¯å®éªŒ2åˆ†é’Ÿ)")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_plants = len(plant_files)
    completed_plants = 0
    failed_plants = 0
    skipped_plants = 0
    partial_plants = 0
    
    start_time = time.time()
    
    for i, (plant_id, data_file) in enumerate(plant_files, 1):
        print(f"\nğŸ“Š è¿›åº¦: {i}/{total_plants} ({i/total_plants*100:.1f}%)")
        
        # æ£€æŸ¥ç»“æœçŠ¶æ€
        has_complete_results, _ = check_existing_results(plant_id)
        is_complete, missing_experiments, existing_count = check_partial_results(plant_id)
        
        if has_complete_results and not force_rerun:
            print(f"â­ï¸  å‚ {plant_id} å·²æœ‰å®Œæ•´ç»“æœï¼Œè·³è¿‡")
            skipped_plants += 1
            continue
        elif is_complete and not force_rerun:
            print(f"â­ï¸  å‚ {plant_id} æ‰€æœ‰å®éªŒå·²å®Œæˆï¼Œè·³è¿‡")
            skipped_plants += 1
            continue
        elif existing_count > 0:
            print(f"ğŸ”„ å‚ {plant_id} éƒ¨åˆ†å®Œæˆ ({existing_count} ä¸ªå®éªŒ)ï¼Œç»§ç»­è¿è¡Œ")
            partial_plants += 1
        
        # è¿è¡Œå®éªŒ
        success = run_plant_experiments(plant_id, data_file, force_rerun)
        
        if success:
            completed_plants += 1
        else:
            failed_plants += 1
        
        # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
        print(f"\nğŸ“ˆ å½“å‰ç»Ÿè®¡:")
        print(f"   å·²å®Œæˆ: {completed_plants}")
        print(f"   éƒ¨åˆ†å®Œæˆ: {partial_plants}")
        print(f"   å·²è·³è¿‡: {skipped_plants}")
        print(f"   å¤±è´¥: {failed_plants}")
        print(f"   å‰©ä½™: {total_plants - completed_plants - partial_plants - skipped_plants - failed_plants}")
    
    # æœ€ç»ˆç»Ÿè®¡
    end_time = time.time()
    total_duration = end_time - start_time
    
    print(f"\nğŸ‰ æ‰€æœ‰å‚å®éªŒå®Œæˆ!")
    print("=" * 80)
    print(f"æ€»å‚æ•°: {total_plants}")
    print(f"å·²å®Œæˆ: {completed_plants}")
    print(f"éƒ¨åˆ†å®Œæˆ: {partial_plants}")
    print(f"å·²è·³è¿‡: {skipped_plants}")
    print(f"å¤±è´¥: {failed_plants}")
    print(f"æ€»è€—æ—¶: {total_duration/3600:.1f}å°æ—¶")
    print(f"å¹³å‡æ¯å‚: {total_duration/total_plants/60:.1f}åˆ†é’Ÿ")

def analyze_results():
    """åˆ†ææ‰€æœ‰å‚çš„ç»“æœ"""
    
    print("\nğŸ“Š åˆ†ææ‰€æœ‰å‚çš„ç»“æœ...")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
    drive_dir = '/content/drive/MyDrive/Solar PV electricity/results'
    local_dir = 'result'
    
    result_dirs = []
    if os.path.exists(drive_dir):
        result_dirs.append(drive_dir)
    if os.path.exists(local_dir):
        result_dirs.append(local_dir)
    
    all_results = []
    
    for result_dir in result_dirs:
        # æŸ¥æ‰¾æ‰€æœ‰summary.csvæ–‡ä»¶
        summary_files = glob.glob(os.path.join(result_dir, '**/summary.csv'), recursive=True)
        
        for file in summary_files:
            try:
                df = pd.read_csv(file)
                df['result_file'] = file
                all_results.append(df)
            except Exception as e:
                print(f"âŒ è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    if not all_results:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
        return
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)
    
    print(f"âœ… æ‰¾åˆ° {len(combined_df)} ä¸ªå®éªŒç»“æœ")
    
    # æŒ‰å‚ç»Ÿè®¡
    plant_stats = combined_df.groupby('plant_id').size().sort_values(ascending=False)
    print(f"\nğŸ“ˆ å„å‚å®éªŒæ•°é‡:")
    print(plant_stats.head(10))
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    model_stats = combined_df.groupby('model').size()
    print(f"\nğŸ¤– å„æ¨¡å‹å®éªŒæ•°é‡:")
    print(model_stats)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = 'all_plants_analysis.csv'
    combined_df.to_csv(analysis_file, index=False)
    print(f"\nğŸ’¾ åˆ†æç»“æœä¿å­˜åˆ°: {analysis_file}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œæ‰€æœ‰132ä¸ªå‚çš„å®Œæ•´å®éªŒ')
    parser.add_argument('--analyze', action='store_true', help='åªåˆ†æç»“æœï¼Œä¸è¿è¡Œå®éªŒ')
    parser.add_argument('--plant_id', type=str, help='åªè¿è¡ŒæŒ‡å®šå‚')
    parser.add_argument('--data_file', type=str, help='æŒ‡å®šæ•°æ®æ–‡ä»¶')
    parser.add_argument('--force_rerun', action='store_true', help='å¼ºåˆ¶é‡æ–°è¿è¡Œæ‰€æœ‰å®éªŒ')
    
    args = parser.parse_args()
    
    if args.analyze:
        analyze_results()
    elif args.plant_id and args.data_file:
        run_plant_experiments(args.plant_id, args.data_file, args.force_rerun)
    else:
        run_all_plants(args.force_rerun)
