#!/usr/bin/env python3
"""
Season and Hour Analysiså®éªŒè¿è¡Œè„šæœ¬
åœ¨100ä¸ªå‚ä¸Šè¿è¡Œseason and hour analysiså®éªŒï¼Œç»“æœä¿å­˜åˆ°Google Drive
æ¯ä¸ªå‚è¿›è¡Œ8ä¸ªå®éªŒï¼Œä¿å­˜prediction.csvå’Œsummary.csv
"""

import os
import sys
import time
import subprocess
import yaml
import pandas as pd
import numpy as np
from pathlib import Path
import re
import uuid
from datetime import datetime

def check_drive_mount():
    """æ£€æŸ¥Google Driveæ˜¯å¦å·²æŒ‚è½½"""
    drive_path = "/content/drive/MyDrive"
    if os.path.exists(drive_path):
        print("âœ… Google Driveå·²æŒ‚è½½")
        return True
    else:
        print("âŒ Google DriveæœªæŒ‚è½½ï¼Œè¯·å…ˆæŒ‚è½½Drive")
        return False

def get_data_files():
    """æ‰«ædataç›®å½•ï¼Œè·å–å‰100ä¸ªé¡¹ç›®çš„CSVæ–‡ä»¶"""
    data_dir = "data"
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    csv_files = []
    for file in os.listdir(data_dir):
        if file.startswith("Project") and file.endswith(".csv"):
            project_id = file.replace("Project", "").replace(".csv", "")
            csv_files.append((project_id, os.path.join(data_dir, file)))
    
    # æŒ‰é¡¹ç›®IDæ’åºï¼Œå–å‰100ä¸ª
    csv_files.sort(key=lambda x: int(x[0]))
    return csv_files[:100]

def get_season_hour_config_files():
    """è·å–æ‰€æœ‰season and hour analysisé…ç½®æ–‡ä»¶"""
    config_dir = "season_and_hour_analysis/configs"
    all_config_files = []
    
    # éœ€è¦è·³è¿‡çš„éå®éªŒé…ç½®æ–‡ä»¶
    skip_files = {
        'season_hour_index.yaml',
        'season_hour_global_index.yaml', 
        'index.yaml',
        'README.yaml',
        'readme.yaml'
    }
    
    if os.path.exists(config_dir):
        for project_dir in os.listdir(config_dir):
            project_path = os.path.join(config_dir, project_dir)
            if os.path.isdir(project_path):
                for config_file in os.listdir(project_path):
                    if config_file.endswith('.yaml') and config_file not in skip_files:
                        all_config_files.append(os.path.join(project_path, config_file))
    
    return all_config_files

def get_completed_experiments_for_project(project_id, drive_path):
    """è·å–æŒ‡å®šé¡¹ç›®å·²å®Œæˆçš„season and hour analysiså®éªŒåˆ—è¡¨"""
    completed_configs = set()
    
    try:
        project_summary = os.path.join(drive_path, f"{project_id}_summary.csv")
        if os.path.exists(project_summary):
            df = pd.read_csv(project_summary)
            if 'config_file' in df.columns:
                completed_configs = set(df['config_file'].tolist())
                print(f"ğŸ“Š é¡¹ç›® {project_id} å·²å®Œæˆseason and hour analysiså®éªŒ: {len(completed_configs)} ä¸ª")
            else:
                print(f"âš ï¸ é¡¹ç›® {project_id} summaryæ–‡ä»¶ç¼ºå°‘config_fileåˆ—")
        else:
            print(f"ğŸ“„ é¡¹ç›® {project_id} season and hour analysisç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–é¡¹ç›® {project_id} season and hour analysisç»“æœæ–‡ä»¶: {e}")
    
    return completed_configs

def run_experiment(config_file, data_file, project_id):
    """è¿è¡Œå•ä¸ªseason and hour analysiså®éªŒ"""
    try:
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„data_path
        config['data_path'] = data_file
        config['plant_id'] = project_id
        
        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨UUIDé¿å…å†²çªï¼‰
        temp_config = f"temp_season_hour_config_{project_id}_{uuid.uuid4().hex[:8]}.yaml"
        with open(temp_config, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        # è¿è¡Œå®éªŒå¹¶è®°å½•æ—¶é—´
        cmd = ['python', 'main.py', '--config', temp_config]
        start_time = time.time()
        
        # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60åˆ†é’Ÿ
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
        duration = time.time() - start_time
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_config):
            os.remove(temp_config)
        
        if result.returncode == 0:
            return True, result.stdout, result.stderr, duration, config
        else:
            return False, result.stdout, result.stderr, duration, config
            
    except subprocess.TimeoutExpired:
        # å¤„ç†è¶…æ—¶
        if os.path.exists(temp_config):
            os.remove(temp_config)
        return False, "", "å®éªŒè¶…æ—¶ï¼ˆ60åˆ†é’Ÿï¼‰", 3600.0, {}
    except Exception as e:
        return False, "", str(e), 0.0, {}

def parse_experiment_output(output, config_file, duration, config):
    """è§£æseason and hour analysiså®éªŒè¾“å‡ºï¼Œæå–ç»“æœ"""
    try:
        # æå–æ¨¡å‹ä¿¡æ¯
        model_name = config.get('model', 'Unknown')
        complexity = config.get('model_complexity', 'unknown')
        
        # æå–å®éªŒå‚æ•°
        weather_level = config.get('weather_category', 'unknown')
        lookback_hours = config.get('past_hours', 24)
        complexity_level = complexity.replace('level', '') if 'level' in complexity else 'unknown'
        dataset_scale = '80%'
        
        # æå–è®­ç»ƒå‚æ•°
        use_pv = config.get('use_pv', False)
        use_hist_weather = config.get('use_hist_weather', False)
        use_forecast = config.get('use_forecast', False)
        time_encoding = config.get('use_time_encoding', False)
        past_days = config.get('past_days', 1)
        use_ideal_nwp = config.get('use_ideal_nwp', False)
        selected_features = config.get('selected_weather_features', [])
        
        # æå–æŒ‡æ ‡
        mse_match = re.search(r'mse=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        rmse_match = re.search(r'rmse=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        mae_match = re.search(r'mae=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        r_square_match = re.search(r'r_square=([+-]?[0-9]*\.?[0-9]+(?:[eE][+-]?[0-9]+)?)', output)
        
        # è®¡ç®—æŒ‡æ ‡
        mse = float(mse_match.group(1)) if mse_match else 0.0
        rmse = float(rmse_match.group(1)) if rmse_match else 0.0
        mae = float(mae_match.group(1)) if mae_match else 0.0
        r_square = float(r_square_match.group(1)) if r_square_match else 0.0
        
        # åˆå§‹åŒ–é¢å¤–å­—æ®µ
        inference_time = 0.0
        param_count = 0
        samples_count = 0
        best_epoch = 0
        final_lr = 0.0
        nrmse = 0.0
        smape = 0.0
        gpu_memory_used = 0
        
        # ä½¿ç”¨METRICSæ ‡ç­¾æå–é¢å¤–ä¿¡æ¯
        for line in output.split('\n'):
            if "[METRICS]" in line:
                metrics_in_line = re.findall(r'(\w+)=([0-9.-]+)', line)
                for key, value_str in metrics_in_line:
                    try:
                        if key == 'inference_time':
                            inference_time = float(value_str)
                        elif key == 'param_count':
                            param_count = int(float(value_str))
                        elif key == 'samples_count':
                            samples_count = int(float(value_str))
                        elif key == 'best_epoch':
                            if value_str.lower() == 'nan':
                                best_epoch = 0
                            else:
                                best_epoch = int(float(value_str))
                        elif key == 'final_lr':
                            if value_str.lower() == 'nan':
                                final_lr = 0.0
                            else:
                                final_lr = float(value_str)
                        elif key == 'nrmse':
                            nrmse = float(value_str)
                        elif key == 'smape':
                            smape = float(value_str)
                        elif key == 'gpu_memory_used':
                            gpu_memory_used = int(float(value_str))
                    except Exception as e:
                        print(f"ğŸ” è°ƒè¯•ï¼š{key}æå–å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ä»METRICSä¸­æå–åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
        if inference_time == 0.0:
            inference_match = re.search(r'Inference time: ([\d.]+)s', output)
            inference_time = float(inference_match.group(1)) if inference_match else 0.0
        
        if param_count == 0:
            param_match = re.search(r'Total parameters: ([\d,]+)', output)
            param_count = int(param_match.group(1).replace(',', '')) if param_match else 0
        
        if samples_count == 0:
            samples_match = re.search(r'Training samples: (\d+)', output)
            samples_count = int(samples_match.group(1)) if samples_match else 0
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹
        is_dl_model = model_name in ['LSTM', 'GRU', 'Transformer', 'TCN']
        has_learning_rate = is_dl_model or model_name in ['XGB', 'LGBM']
        
        if best_epoch == 0 and is_dl_model:
            epoch_match = re.search(r'Best epoch: (\d+)', output)
            best_epoch = int(epoch_match.group(1)) if epoch_match else 0
        
        if final_lr == 0.0 and is_dl_model:
            lr_match = re.search(r'Final LR: ([\d.]+)', output)
            final_lr = float(lr_match.group(1)) if lr_match else 0.0
        
        if gpu_memory_used == 0:
            gpu_memory_match = re.search(r'GPU memory used: ([\d.]+)MB', output)
            gpu_memory_used = int(float(gpu_memory_match.group(1))) if gpu_memory_match else 0
        
        # è®¡ç®—NRMSEå’ŒSMAPEï¼ˆå¦‚æœæœªä»METRICSä¸­æå–ï¼‰
        if nrmse == 0.0 and mae > 0:
            nrmse = (rmse / (mae + 1e-8)) * 100
        
        if smape == 0.0 and mae > 0:
            smape = (2 * mae / (mae + 1e-8)) * 100
        
        # åˆ›å»ºç»“æœè¡Œ
        result_row = {
            'model': model_name,
            'weather_level': weather_level,
            'lookback_hours': lookback_hours,
            'complexity_level': complexity_level,
            'dataset_scale': dataset_scale,
            'use_pv': use_pv,
            'use_hist_weather': use_hist_weather,
            'use_forecast': use_forecast,
            'use_time_encoding': time_encoding,
            'past_days': past_days,
            'use_ideal_nwp': use_ideal_nwp,
            'selected_weather_features': str(selected_features),
            'epochs': config.get('epochs', 80 if complexity_level == '4' else 50) if is_dl_model else 0,
            'batch_size': config.get('train_params', {}).get('batch_size', 64) if is_dl_model else 0,
            'learning_rate': config.get('train_params', {}).get('learning_rate', 0.001) if has_learning_rate else 0.0,
            'train_time_sec': round(duration, 4),
            'inference_time_sec': inference_time,
            'param_count': param_count,
            'samples_count': samples_count,
            'best_epoch': best_epoch if is_dl_model else 0,
            'final_lr': final_lr if is_dl_model else 0.0,
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'nrmse': nrmse,
            'r_square': r_square,
            'smape': smape,
            'gpu_memory_used': gpu_memory_used,
            'config_file': os.path.basename(config_file)
        }
        
        return result_row
        
    except Exception as e:
        print(f"âš ï¸ è§£æseason and hour analysiså®éªŒç»“æœå¤±è´¥: {e}")
        return None

def create_project_summary_csv(project_id, drive_path):
    """ä¸ºé¡¹ç›®åˆ›å»ºseason and hour analysis summary CSVæ–‡ä»¶"""
    csv_file = os.path.join(drive_path, f"{project_id}_summary.csv")
    
    if not os.path.exists(csv_file):
        # åˆ›å»ºCSVæ–‡ä»¶å¤´
        columns = [
            'model', 'weather_level', 'lookback_hours', 'complexity_level', 'dataset_scale',
            'use_pv', 'use_hist_weather', 'use_forecast', 'use_time_encoding', 'past_days',
            'use_ideal_nwp', 'selected_weather_features', 'epochs', 'batch_size', 'learning_rate',
            'train_time_sec', 'inference_time_sec', 'param_count', 'samples_count', 'best_epoch',
            'final_lr', 'mse', 'rmse', 'mae', 'nrmse', 'r_square', 'smape', 'gpu_memory_used', 'config_file'
        ]
        
        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_file, index=False)
        print(f"ğŸ“„ åˆ›å»ºé¡¹ç›®season and hour analysis summary CSVæ–‡ä»¶: {csv_file}")
        return True
    else:
        print(f"ğŸ“„ é¡¹ç›®season and hour analysis summary CSVæ–‡ä»¶å·²å­˜åœ¨: {csv_file}")
        return True

def save_single_result_to_summary_csv(result_row, project_id, drive_path):
    """ä¿å­˜å•ä¸ªseason and hour analysisç»“æœåˆ°é¡¹ç›®summary CSVæ–‡ä»¶"""
    try:
        csv_file = os.path.join(drive_path, f"{project_id}_summary.csv")
        
        # è¯»å–ç°æœ‰CSV
        if os.path.exists(csv_file):
            df = pd.read_csv(csv_file)
        else:
            df = pd.DataFrame()
        
        # æ·»åŠ æ–°è¡Œ
        new_row_df = pd.DataFrame([result_row])
        df = pd.concat([df, new_row_df], ignore_index=True)
        
        # ä¿å­˜CSV
        df.to_csv(csv_file, index=False)
        print(f"ğŸ’¾ season and hour analysisç»“æœå·²ä¿å­˜åˆ°summary: {csv_file}")
        print(f"ğŸ“Š CSVæ–‡ä»¶å½“å‰è¡Œæ•°: {len(df)}")
        print(f"ğŸ“Š æœ€æ–°å®éªŒ: {result_row['model']} - {result_row['mse']:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜season and hour analysisç»“æœå¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è°ƒè¯•: è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return False

def extract_predictions_from_output(output, config):
    """ä»å®éªŒè¾“å‡ºä¸­æå–é¢„æµ‹ç»“æœå’Œæ—¥æœŸä¿¡æ¯"""
    try:
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„main.pyè¾“å‡ºæ ¼å¼æ¥è§£æ
        # å‡è®¾è¾“å‡ºä¸­åŒ…å«é¢„æµ‹ç»“æœå’Œæ—¥æœŸä¿¡æ¯
        # å®é™…å®ç°éœ€è¦æ ¹æ®main.pyçš„å…·ä½“è¾“å‡ºæ ¼å¼æ¥è°ƒæ•´
        
        # ä¸´æ—¶è¿”å›ç©ºæ•°æ®ï¼Œå®é™…éœ€è¦æ ¹æ®main.pyçš„è¾“å‡ºæ ¼å¼æ¥è§£æ
        predictions = []
        dates = []
        
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„è¾“å‡ºæ ¼å¼æ¥è§£æé¢„æµ‹ç»“æœå’Œæ—¥æœŸ
        # ä¾‹å¦‚ä»CSVæ–‡ä»¶æˆ–è¾“å‡ºæ–‡æœ¬ä¸­æå–
        
        return predictions, dates
        
    except Exception as e:
        print(f"âš ï¸ æå–é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        return [], []

def save_predictions_csv(predictions, dates, project_id, model_name, drive_path):
    """ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶"""
    try:
        if not predictions or not dates:
            print(f"âš ï¸ æ²¡æœ‰é¢„æµ‹æ•°æ®å¯ä¿å­˜: {project_id}_{model_name}")
            return False
        
        # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
        pred_df = pd.DataFrame({
            'date': dates,
            'ground_truth': predictions.get('ground_truth', []),
            'prediction': predictions.get('prediction', []),
            'model': model_name,
            'project_id': project_id
        })
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        pred_file = os.path.join(drive_path, f"{project_id}_prediction.csv")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®
        if os.path.exists(pred_file):
            existing_df = pd.read_csv(pred_file)
            pred_df = pd.concat([existing_df, pred_df], ignore_index=True)
        
        pred_df.to_csv(pred_file, index=False)
        print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ SolarPVé¡¹ç›® - Season and Hour Analysiså®éªŒè„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥Google Drive
    if not check_drive_mount():
        return
    
    # è®¾ç½®è·¯å¾„ - ä½¿ç”¨æŒ‡å®šçš„Driveè·¯å¾„
    drive_path = "/content/drive/MyDrive/Solar PV electricity/hour and season analysis"
    os.makedirs(drive_path, exist_ok=True)
    
    # è·å–æ•°æ®æ–‡ä»¶
    print("ğŸ“ æ‰«ææ•°æ®æ–‡ä»¶...")
    data_files = get_data_files()
    if not data_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼ˆå‰100ä¸ªå‚ï¼‰")
    
    # è·å–season and hour analysisé…ç½®æ–‡ä»¶
    print("ğŸ“ æ‰«æseason and hour analysisé…ç½®æ–‡ä»¶...")
    config_files = get_season_hour_config_files()
    print(f"ğŸ“Š æ‰¾åˆ° {len(config_files)} ä¸ªseason and hour analysisé…ç½®æ–‡ä»¶")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆé…ç½®æ–‡ä»¶
    if len(config_files) < len(data_files) * 8:
        print("ğŸ”§ season and hour analysisé…ç½®æ–‡ä»¶ä¸è¶³ï¼Œæ­£åœ¨ç”Ÿæˆ...")
        try:
            result = subprocess.run([
                'python', 'season_and_hour_analysis/scripts/generate_season_hour_configs.py'
            ], capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print("âœ… season and hour analysisé…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
                config_files = get_season_hour_config_files()
                print(f"ğŸ“Š ç°åœ¨æœ‰ {len(config_files)} ä¸ªseason and hour analysisé…ç½®æ–‡ä»¶")
            else:
                print(f"âŒ season and hour analysisé…ç½®æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return
        except Exception as e:
            print(f"âŒ season and hour analysisé…ç½®æ–‡ä»¶ç”Ÿæˆå¼‚å¸¸: {e}")
            return
    
    # å¼€å§‹å®éªŒ
    all_results = []
    total_experiments = 0
    completed_experiments = 0
    failed_experiments = 0
    
    for project_idx, (project_id, data_file) in enumerate(data_files, 1):
        print(f"\nğŸš€ å¼€å§‹é¡¹ç›® {project_id} çš„season and hour analysiså®éªŒ ({project_idx}/{len(data_files)})")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")
        
        # è·å–è¯¥é¡¹ç›®çš„season and hour analysisé…ç½®æ–‡ä»¶
        project_configs = [cf for cf in config_files if f"/{project_id}/" in cf]
        print(f"ğŸ“Š æ‰¾åˆ° {len(project_configs)} ä¸ªseason and hour analysisé…ç½®æ–‡ä»¶")
        
        if not project_configs:
            print(f"âš ï¸ é¡¹ç›® {project_id} æ²¡æœ‰season and hour analysisé…ç½®æ–‡ä»¶ï¼Œè·³è¿‡")
            continue
        
        # åˆ›å»ºé¡¹ç›®summary CSVæ–‡ä»¶
        if not create_project_summary_csv(project_id, drive_path):
            print(f"âŒ æ— æ³•ä¸ºé¡¹ç›® {project_id} åˆ›å»ºseason and hour analysis summary CSVæ–‡ä»¶")
            continue
        
        # è·å–è¯¥é¡¹ç›®å·²å®Œæˆçš„season and hour analysiså®éªŒ
        completed_configs = get_completed_experiments_for_project(project_id, drive_path)
        
        project_results = []
        skipped_count = 0
        
        for config_file in project_configs:
            config_name = os.path.basename(config_file)
            total_experiments += 1
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if config_name in completed_configs:
                skipped_count += 1
                if skipped_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè·³è¿‡çš„å®éªŒ
                    print(f"â­ï¸ è·³è¿‡å·²å®Œæˆå®éªŒ: {config_name}")
                continue
            
            print(f"\nğŸ”„ è¿è¡Œseason and hour analysiså®éªŒ: {config_name}")
            
            # è¿è¡Œå®éªŒ
            success, stdout, stderr, duration, config = run_experiment(config_file, data_file, project_id)
            
            if success:
                # è§£æç»“æœ
                print(f"ğŸ” è°ƒè¯•: å¼€å§‹è§£æseason and hour analysiså®éªŒç»“æœ: {config_name}")
                result_row = parse_experiment_output(stdout, config_file, duration, config)
                if result_row:
                    project_results.append(result_row)
                    completed_experiments += 1
                    print(f"âœ… season and hour analysiså®éªŒå®Œæˆ: {config_name} ({duration:.1f}s) - MSE: {result_row['mse']:.4f}")
                    print(f"ğŸ” è°ƒè¯•: è§£ææˆåŠŸï¼Œç»“æœå­—æ®µ: {list(result_row.keys())}")
                    print(f"ğŸ” è°ƒè¯•: å½“å‰project_resultsæ•°é‡: {len(project_results)}")
                    
                    # ç«‹å³ä¿å­˜åˆ°é¡¹ç›®summary CSV
                    print(f"ğŸ’¾ ç«‹å³ä¿å­˜season and hour analysisç»“æœåˆ°é¡¹ç›®summary CSV...")
                    save_single_result_to_summary_csv(result_row, project_id, drive_path)
                    print(f"âœ… season and hour analysisç»“æœå·²ä¿å­˜åˆ°é¡¹ç›®summary CSV")
                    
                    # æ³¨æ„ï¼šprediction.csvå·²ç»åœ¨main.pyä¸­é€šè¿‡save_season_hour_results()å‡½æ•°åˆ›å»º
                    # è¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†ï¼Œå› ä¸ºsave_season_hour_results()ä¼šç›´æ¥ä¿å­˜åˆ°Drive
                    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²åœ¨main.pyä¸­ä¿å­˜åˆ°Drive")
                else:
                    failed_experiments += 1
                    print(f"âš ï¸ æ— æ³•è§£æseason and hour analysiså®éªŒç»“æœ: {config_name}")
                    print(f"ğŸ” è°ƒè¯•: å®éªŒè¾“å‡ºå‰500å­—ç¬¦: {stdout[:500]}")
            else:
                failed_experiments += 1
                print(f"âŒ season and hour analysiså®éªŒå¤±è´¥: {config_name}")
                print(f"   é”™è¯¯: {stderr}")
                print(f"ğŸ” è°ƒè¯•: æ ‡å‡†è¾“å‡º: {stdout[:200]}")
        
        if skipped_count > 5:
            print(f"â­ï¸ ... è¿˜æœ‰ {skipped_count - 5} ä¸ªå·²å®Œæˆçš„season and hour analysiså®éªŒè¢«è·³è¿‡")
        
        # é¡¹ç›®å®Œæˆç»Ÿè®¡
        print(f"âœ… é¡¹ç›® {project_id} season and hour analysiså®Œæˆ!")
        print(f"ğŸ“Š é¡¹ç›® {project_id} season and hour analysisç»Ÿè®¡:")
        print(f"   æ€»å®éªŒ: {len(project_configs)}")
        print(f"   è·³è¿‡: {skipped_count}")
        print(f"   å®Œæˆ: {len(project_results)}")
        print(f"   å¤±è´¥: {len(project_configs) - skipped_count - len(project_results)}")
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰season and hour analysiså®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æ€»å®éªŒæ•°: {total_experiments}")
    print(f"âœ… å®Œæˆ: {completed_experiments}")
    print(f"âŒ å¤±è´¥: {failed_experiments}")
    print(f"â­ï¸ è·³è¿‡: {total_experiments - completed_experiments - failed_experiments}")
    
    if all_results:
        print(f"ğŸ’¾ æ€»å…±ä¿å­˜äº† {len(all_results)} ä¸ªseason and hour analysisç»“æœ")
        print(f"ğŸ“ season and hour analysisç»“æœä¿å­˜åœ¨: {drive_path}")

if __name__ == "__main__":
    main()
