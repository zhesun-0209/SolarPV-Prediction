#!/usr/bin/env python3
"""
A100æ€§èƒ½æµ‹è¯•è„šæœ¬
å¿«é€Ÿæµ‹è¯•ç³»ç»Ÿæ€§èƒ½å¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®
"""

import torch
import time
import psutil
import subprocess
import sys
from pathlib import Path

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print("ğŸ¯ GPUæ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        print(f"   GPU {i}: {props.name} ({memory_gb:.1f}GB)")
        
        if "A100" in props.name:
            print("   ğŸš€ A100æ£€æµ‹åˆ°ï¼å¯ç”¨é«˜æ€§èƒ½æ¨¡å¼")
            return True
    
    print("âš ï¸ æœªæ£€æµ‹åˆ°A100ï¼Œå°†ä½¿ç”¨æ ‡å‡†é«˜æ€§èƒ½æ¨¡å¼")
    return True

def test_memory_performance():
    """æµ‹è¯•å†…å­˜æ€§èƒ½"""
    print("\nğŸ’¾ å†…å­˜æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    memory = psutil.virtual_memory()
    memory_gb = memory.total / 1024**3
    
    print(f"æ€»å†…å­˜: {memory_gb:.1f}GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available / 1024**3:.1f}GB")
    print(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    if memory_gb >= 128:
        print("âœ… å¤§å†…å­˜ç³»ç»Ÿï¼Œæ”¯æŒé«˜å¹¶è¡Œ")
        return "high"
    elif memory_gb >= 64:
        print("âœ… ä¸­ç­‰å†…å­˜ç³»ç»Ÿï¼Œæ”¯æŒä¸­ç­‰å¹¶è¡Œ")
        return "medium"
    else:
        print("âš ï¸ å°å†…å­˜ç³»ç»Ÿï¼Œå»ºè®®å‡å°‘å¹¶è¡Œæ•°")
        return "low"

def test_cpu_performance():
    """æµ‹è¯•CPUæ€§èƒ½"""
    print("\nğŸ’» CPUæ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    cpu_count = psutil.cpu_count()
    cpu_freq = psutil.cpu_freq()
    
    print(f"CPUæ ¸å¿ƒæ•°: {cpu_count}")
    if cpu_freq:
        print(f"CPUé¢‘ç‡: {cpu_freq.current:.0f}MHz")
    
    # ç®€å•CPUæ€§èƒ½æµ‹è¯•
    start_time = time.time()
    for i in range(1000000):
        _ = i * i
    cpu_time = time.time() - start_time
    
    print(f"CPUè®¡ç®—æ€§èƒ½: {cpu_time:.3f}ç§’ (100ä¸‡æ¬¡ä¹˜æ³•)")
    
    if cpu_count >= 32:
        return "high"
    elif cpu_count >= 16:
        return "medium"
    else:
        return "low"

def test_disk_performance():
    """æµ‹è¯•ç£ç›˜æ€§èƒ½"""
    print("\nğŸ’¿ ç£ç›˜æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    # æµ‹è¯•å†™å…¥æ€§èƒ½
    test_file = Path("test_disk_performance.tmp")
    
    start_time = time.time()
    with open(test_file, 'wb') as f:
        f.write(b'0' * 1024 * 1024)  # 1MB
    write_time = time.time() - start_time
    
    start_time = time.time()
    with open(test_file, 'rb') as f:
        f.read()
    read_time = time.time() - start_time
    
    test_file.unlink()  # åˆ é™¤æµ‹è¯•æ–‡ä»¶
    
    print(f"å†™å…¥æ€§èƒ½: {1/write_time:.1f}MB/s")
    print(f"è¯»å–æ€§èƒ½: {1/read_time:.1f}MB/s")
    
    if write_time < 0.01:  # 100MB/s+
        return "high"
    elif write_time < 0.05:  # 20MB/s+
        return "medium"
    else:
        return "low"

def get_optimization_recommendations(gpu_available, memory_level, cpu_level, disk_level):
    """è·å–ä¼˜åŒ–å»ºè®®"""
    print("\nğŸ¯ ä¼˜åŒ–å»ºè®®")
    print("-" * 40)
    
    recommendations = []
    
    # GPUå»ºè®®
    if gpu_available:
        if memory_level == "high":
            recommendations.append("ğŸš€ GPUå¹¶è¡Œæ•°: 16")
            recommendations.append("ğŸ’» CPUå¹¶è¡Œæ•°: 32")
            recommendations.append("ğŸ“¦ æ‰¹å¤„ç†å¤§å°: 128")
        elif memory_level == "medium":
            recommendations.append("ğŸš€ GPUå¹¶è¡Œæ•°: 12")
            recommendations.append("ğŸ’» CPUå¹¶è¡Œæ•°: 24")
            recommendations.append("ğŸ“¦ æ‰¹å¤„ç†å¤§å°: 96")
        else:
            recommendations.append("ğŸš€ GPUå¹¶è¡Œæ•°: 8")
            recommendations.append("ğŸ’» CPUå¹¶è¡Œæ•°: 16")
            recommendations.append("ğŸ“¦ æ‰¹å¤„ç†å¤§å°: 64")
        
        recommendations.append("âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
        recommendations.append("ğŸ”„ å¯ç”¨æ¢¯åº¦ç´¯ç§¯")
    else:
        recommendations.append("ğŸ’» CPUå¹¶è¡Œæ•°: 16")
        recommendations.append("ğŸ“¦ æ‰¹å¤„ç†å¤§å°: 32")
        recommendations.append("âš ï¸ å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿ")
    
    # å†…å­˜å»ºè®®
    if memory_level == "low":
        recommendations.append("âš ï¸ å†…å­˜ä¸è¶³ï¼Œå»ºè®®å‡å°‘å¹¶è¡Œæ•°")
        recommendations.append("ğŸ’¾ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # ç£ç›˜å»ºè®®
    if disk_level == "low":
        recommendations.append("ğŸ’¿ ç£ç›˜æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨SSD")
        recommendations.append("ğŸ“ å‡å°‘æ•°æ®åŠ è½½å™¨çº¿ç¨‹æ•°")
    
    # ç¯å¢ƒå˜é‡å»ºè®®
    recommendations.append("\nğŸ”§ ç¯å¢ƒå˜é‡è®¾ç½®:")
    recommendations.append("export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128")
    recommendations.append("export PYTORCH_CUDNN_V8_API_ENABLED=1")
    recommendations.append("export OMP_NUM_THREADS=16")
    
    for rec in recommendations:
        print(f"   {rec}")

def estimate_completion_time(gpu_available, memory_level, cpu_level):
    """ä¼°ç®—å®Œæˆæ—¶é—´"""
    print("\nâ° å®Œæˆæ—¶é—´é¢„ä¼°")
    print("-" * 40)
    
    total_experiments = 36000  # 100ä¸ªProject Ã— 360ä¸ªå®éªŒ
    
    if gpu_available:
        if memory_level == "high":
            gpu_parallel = 16
            cpu_parallel = 32
            avg_time_per_experiment = 120  # ç§’
        elif memory_level == "medium":
            gpu_parallel = 12
            cpu_parallel = 24
            avg_time_per_experiment = 150  # ç§’
        else:
            gpu_parallel = 8
            cpu_parallel = 16
            avg_time_per_experiment = 200  # ç§’
    else:
        cpu_parallel = 16
        avg_time_per_experiment = 300  # ç§’
    
    # è®¡ç®—æ€»æ—¶é—´
    if gpu_available:
        # GPUå’ŒCPUå¹¶è¡Œè¿è¡Œ
        gpu_experiments = 14400  # 40% æ·±åº¦å­¦ä¹ æ¨¡å‹
        cpu_experiments = 21600  # 60% ä¼ ç»ŸMLæ¨¡å‹
        
        gpu_time = (gpu_experiments * avg_time_per_experiment) / (gpu_parallel * 3600)  # å°æ—¶
        cpu_time = (cpu_experiments * avg_time_per_experiment) / (cpu_parallel * 3600)  # å°æ—¶
        
        total_hours = max(gpu_time, cpu_time)  # å–æœ€å¤§å€¼ï¼Œå› ä¸ºå®ƒä»¬å¹¶è¡Œè¿è¡Œ
    else:
        total_hours = (total_experiments * avg_time_per_experiment) / (cpu_parallel * 3600)
    
    total_days = total_hours / 24
    
    print(f"æ€»å®éªŒæ•°: {total_experiments:,}")
    if gpu_available:
        print(f"GPUå¹¶è¡Œæ•°: {gpu_parallel}")
        print(f"CPUå¹¶è¡Œæ•°: {cpu_parallel}")
    else:
        print(f"CPUå¹¶è¡Œæ•°: {cpu_parallel}")
    
    print(f"é¢„è®¡å®Œæˆæ—¶é—´: {total_hours:.0f} å°æ—¶ ({total_days:.1f} å¤©)")
    print(f"å®éªŒé€Ÿåº¦: {total_experiments / total_hours:.1f} å®éªŒ/å°æ—¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Project1140 A100æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    gpu_available = test_gpu_performance()
    memory_level = test_memory_performance()
    cpu_level = test_cpu_performance()
    disk_level = test_disk_performance()
    
    # è·å–ä¼˜åŒ–å»ºè®®
    get_optimization_recommendations(gpu_available, memory_level, cpu_level, disk_level)
    
    # ä¼°ç®—å®Œæˆæ—¶é—´
    estimate_completion_time(gpu_available, memory_level, cpu_level)
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æ¨èå¯åŠ¨å‘½ä»¤:")
    
    if gpu_available:
        print("   ./run_a100_optimized.sh")
        print("   # æˆ–")
        print("   python scripts/run_high_performance_experiments.py")
    else:
        print("   ./run_100_projects.sh")
        print("   # æˆ–")
        print("   python scripts/run_multi_project_experiments.py")
    
    print("\nğŸ“Š æ€§èƒ½ç›‘æ§:")
    print("   ./monitor_a100_experiment.sh")
    
    print("\nğŸ” è¯¦ç»†æ€§èƒ½åˆ†æ:")
    print("   python scripts/performance_estimator.py")

if __name__ == "__main__":
    main()
