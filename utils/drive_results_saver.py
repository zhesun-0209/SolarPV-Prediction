#!/usr/bin/env python3
"""
Google Driveç»“æœä¿å­˜å™¨
æ”¯æŒå®æ—¶è¿½åŠ å†™å…¥CSVç»“æœï¼Œæ–­ç‚¹ç»­è®­åŠŸèƒ½
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
try:
    from typing import Optional
except ImportError:
    Optional = None
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DriveResultsSaver:
    """Google Driveç»“æœä¿å­˜å™¨"""
    
    def __init__(self, drive_path: str = "/content/drive/MyDrive/Solar PV electricity/ablation results"):
        self.drive_path = Path(drive_path)
        self.drive_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºä¸´æ—¶æœ¬åœ°ç¼“å­˜ç›®å½•
        self.local_cache_dir = Path("./temp_drive_cache")
        self.local_cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Driveç»“æœä¿å­˜å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"Driveè·¯å¾„: {self.drive_path}")
        logger.info(f"æœ¬åœ°ç¼“å­˜: {self.local_cache_dir}")
    
    def get_project_csv_path(self, project_id: str) -> Path:
        """è·å–Projectçš„CSVç»“æœæ–‡ä»¶è·¯å¾„"""
        return self.drive_path / f"{project_id}.csv"
    
    def get_local_cache_path(self, project_id: str) -> Path:
        """è·å–æœ¬åœ°ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.local_cache_dir / f"{project_id}.csv"
    
    def load_existing_results(self, project_id: str) -> pd.DataFrame:
        """åŠ è½½å·²å­˜åœ¨çš„ç»“æœ"""
        csv_path = self.get_project_csv_path(project_id)
        local_cache_path = self.get_local_cache_path(project_id)
        
        # ä¼˜å…ˆä»DriveåŠ è½½
        if csv_path.exists():
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = csv_path.stat().st_size
                if file_size == 0:
                    logger.info(f"Driveæ–‡ä»¶ {project_id} ä¸ºç©ºï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
                    return pd.DataFrame()
                
                df = pd.read_csv(csv_path)
                if df.empty:
                    logger.info(f"Driveæ–‡ä»¶ {project_id} ä¸ºç©ºDataFrame")
                    return pd.DataFrame()
                
                # åŒæ—¶ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
                df.to_csv(local_cache_path, index=False)
                logger.info(f"ä»DriveåŠ è½½ {project_id} ç»“æœ: {len(df)} æ¡è®°å½•")
                return df
            except Exception as e:
                logger.warning(f"Driveæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                # å¦‚æœDriveæ–‡ä»¶æŸåï¼Œå°è¯•åˆ é™¤å¹¶é‡æ–°å¼€å§‹
                try:
                    csv_path.unlink()
                    logger.info(f"å·²åˆ é™¤æŸåçš„Driveæ–‡ä»¶: {csv_path}")
                except:
                    pass
        
        # å¦‚æœDriveæ²¡æœ‰ï¼Œå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
        if local_cache_path.exists():
            try:
                df = pd.read_csv(local_cache_path)
                logger.info(f"ä»æœ¬åœ°ç¼“å­˜åŠ è½½ {project_id} ç»“æœ: {len(df)} æ¡è®°å½•")
                return df
            except Exception as e:
                logger.warning(f"æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        # è¿”å›ç©ºçš„DataFrame
        logger.info(f"æœªæ‰¾åˆ° {project_id} çš„å·²æœ‰ç»“æœï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
        return pd.DataFrame()
    
    def get_completed_experiments(self, project_id: str) -> set:
        """è·å–å·²å®Œæˆçš„å®éªŒé…ç½®åç§°"""
        df = self.load_existing_results(project_id)
        if df.empty:
            return set()
        
        if 'config_name' in df.columns:
            completed = set(df['config_name'].tolist())
            logger.info(f"{project_id} å·²å®Œæˆå®éªŒ: {len(completed)} ä¸ª")
            return completed
        
        return set()
    
    def save_experiment_result(self, project_id: str, result_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å•ä¸ªå®éªŒç»“æœåˆ°CSV"""
        try:
            # åŠ è½½å·²æœ‰ç»“æœ
            df = self.load_existing_results(project_id)
            
            # å‡†å¤‡æ–°ç»“æœæ•°æ®
            new_row = {
                'project_id': project_id,
                'config_name': result_data.get('config_name', ''),
                'status': result_data.get('status', 'completed'),
                'timestamp': datetime.now().isoformat(),
                'duration': result_data.get('duration', 0),
                
                # æ€§èƒ½æŒ‡æ ‡
                'mae': result_data.get('mae', np.nan),
                'rmse': result_data.get('rmse', np.nan),
                'r2': result_data.get('r2', np.nan),
                'mape': result_data.get('mape', np.nan),
                
                # è®­ç»ƒä¿¡æ¯
                'train_time_sec': result_data.get('train_time_sec', np.nan),
                'inference_time_sec': result_data.get('inference_time_sec', np.nan),
                'param_count': result_data.get('param_count', np.nan),
                'samples_count': result_data.get('samples_count', np.nan),
                
                # é…ç½®ä¿¡æ¯
                'model': result_data.get('model', ''),
                'model_complexity': result_data.get('model_complexity', ''),
                'input_category': result_data.get('input_category', ''),
                'lookback_hours': result_data.get('lookback_hours', np.nan),
                'use_time_encoding': result_data.get('use_time_encoding', False),
                
                # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                'error_message': result_data.get('error_message', '')
            }
            
            # è¿½åŠ æ–°è¡Œ
            new_df = pd.DataFrame([new_row])
            df = pd.concat([df, new_df], ignore_index=True)
            
            # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
            local_cache_path = self.get_local_cache_path(project_id)
            df.to_csv(local_cache_path, index=False)
            
            # ä¿å­˜åˆ°Drive
            drive_path = self.get_project_csv_path(project_id)
            df.to_csv(drive_path, index=False)
            
            logger.info(f"âœ… {project_id} ç»“æœå·²ä¿å­˜: {result_data.get('config_name', 'unknown')}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {project_id} ç»“æœå¤±è´¥: {e}")
            return False
    
    def batch_save_results(self, project_id: str, results: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡ä¿å­˜å¤šä¸ªå®éªŒç»“æœ"""
        saved_count = 0
        
        for result in results:
            if self.save_experiment_result(project_id, result):
                saved_count += 1
        
        logger.info(f"ğŸ“Š {project_id} æ‰¹é‡ä¿å­˜å®Œæˆ: {saved_count}/{len(results)}")
        return saved_count
    
    def get_project_statistics(self, project_id: str) -> Dict[str, Any]:
        """è·å–Projectçš„ç»Ÿè®¡ä¿¡æ¯"""
        df = self.load_existing_results(project_id)
        
        if df.empty:
            return {
                'project_id': project_id,
                'total_experiments': 0,
                'completed_experiments': 0,
                'failed_experiments': 0,
                'completion_rate': 0.0,
                'best_mae': np.nan,
                'best_rmse': np.nan,
                'best_r2': np.nan
            }
        
        completed_df = df[df['status'] == 'completed']
        
        stats = {
            'project_id': project_id,
            'total_experiments': len(df),
            'completed_experiments': len(completed_df),
            'failed_experiments': len(df) - len(completed_df),
            'completion_rate': len(completed_df) / len(df) * 100 if len(df) > 0 else 0.0
        }
        
        if len(completed_df) > 0:
            stats.update({
                'best_mae': completed_df['mae'].min() if 'mae' in completed_df.columns else np.nan,
                'best_rmse': completed_df['rmse'].min() if 'rmse' in completed_df.columns else np.nan,
                'best_r2': completed_df['r2'].max() if 'r2' in completed_df.columns else np.nan,
                'avg_mae': completed_df['mae'].mean() if 'mae' in completed_df.columns else np.nan,
                'avg_rmse': completed_df['rmse'].mean() if 'rmse' in completed_df.columns else np.nan,
                'avg_r2': completed_df['r2'].mean() if 'r2' in completed_df.columns else np.nan
            })
        
        return stats
    
    def get_all_projects_statistics(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰Projectçš„ç»Ÿè®¡ä¿¡æ¯"""
        stats_list = []
        
        # æ‰«ææ‰€æœ‰CSVæ–‡ä»¶
        for csv_file in self.drive_path.glob("*.csv"):
            project_id = csv_file.stem
            stats = self.get_project_statistics(project_id)
            stats_list.append(stats)
        
        if stats_list:
            df = pd.DataFrame(stats_list)
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {len(df)} ä¸ªProject")
            return df
        else:
            logger.info("ğŸ“Š æœªæ‰¾åˆ°ä»»ä½•Projectç»“æœ")
            return pd.DataFrame()
    
    def cleanup_local_cache(self):
        """æ¸…ç†æœ¬åœ°ç¼“å­˜"""
        try:
            import shutil
            shutil.rmtree(self.local_cache_dir)
            self.local_cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info("ğŸ§¹ æœ¬åœ°ç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"æ¸…ç†æœ¬åœ°ç¼“å­˜å¤±è´¥: {e}")
    
    def sync_to_drive(self):
        """åŒæ­¥æ‰€æœ‰æœ¬åœ°ç¼“å­˜åˆ°Drive"""
        synced_count = 0
        
        for cache_file in self.local_cache_dir.glob("*.csv"):
            project_id = cache_file.stem
            drive_path = self.get_project_csv_path(project_id)
            
            try:
                # å¤åˆ¶åˆ°Drive
                import shutil
                shutil.copy2(cache_file, drive_path)
                synced_count += 1
                logger.info(f"ğŸ“¤ åŒæ­¥ {project_id} åˆ°Drive")
            except Exception as e:
                logger.error(f"âŒ åŒæ­¥ {project_id} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ“¤ åŒæ­¥å®Œæˆ: {synced_count} ä¸ªæ–‡ä»¶")
        return synced_count

def test_drive_saver():
    """æµ‹è¯•Driveä¿å­˜å™¨"""
    saver = DriveResultsSaver()
    
    # æµ‹è¯•ä¿å­˜ç»“æœ
    test_result = {
        'config_name': 'Transformer_high_PV_plus_NWP_72h_TE',
        'status': 'completed',
        'duration': 120.5,
        'mae': 0.1234,
        'rmse': 0.1567,
        'r2': 0.8765,
        'mape': 5.4321,
        'train_time_sec': 95.2,
        'inference_time_sec': 0.8,
        'param_count': 1250000,
        'samples_count': 2400,
        'model': 'Transformer',
        'model_complexity': 'high',
        'input_category': 'PV_plus_NWP',
        'lookback_hours': 72,
        'use_time_encoding': True
    }
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    success = saver.save_experiment_result('Project001', test_result)
    print(f"æµ‹è¯•ä¿å­˜ç»“æœ: {success}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = saver.get_project_statistics('Project001')
    print(f"Project001ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    test_drive_saver()
