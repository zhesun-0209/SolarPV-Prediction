import torch
import torch.nn as nn

class TCNModel(nn.Module):
    def __init__(self, hist_dim: int, fcst_dim: int, config: dict):
        super().__init__()
        channels = config['tcn_channels']
        kernel = config['kernel_size']
        self.use_fcst = config.get('use_forecast', False)
        future_hours = config['future_hours']

        # Store channels for later use
        self.channels = channels
        
        # Build TCN encoder on historical sequence (only if hist_dim > 0)
        if hist_dim > 0:
            layers = []
            in_ch = hist_dim
            for out_ch in channels:
                # åŠ¨æ€è°ƒæ•´kernel_sizeï¼Œç¡®ä¿ä¸è¶…è¿‡è¾“å…¥é•¿åº¦
                # è¿™é‡Œä½¿ç”¨min(kernel, 3)æ¥é¿å…kernel_sizeè¿‡å¤§çš„é—®é¢˜
                actual_kernel = min(kernel, 3)
                layers.append(nn.Conv1d(in_ch, out_ch, kernel_size=actual_kernel, padding=actual_kernel // 2))
                layers.append(nn.ReLU())
                in_ch = out_ch
            self.encoder = nn.Sequential(*layers)
        else:
            self.encoder = None

        # Forecast feature projection if enabled
        if self.use_fcst and fcst_dim > 0:
            # ç®€åŒ–çš„é¢„æµ‹ç‰¹å¾å¤„ç†ï¼Œä¸MLæ¨¡å‹ä¿æŒä¸€è‡´
            self.fcst_proj = nn.Linear(fcst_dim * future_hours, channels[-1])
        else:
            self.fcst_proj = None

        # Final prediction head with Softplus
        self.head = nn.Sequential(
            nn.Linear(channels[-1], future_hours),
            nn.Softplus()
        )
        
        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§å¤„ç†
        self.eps = 1e-8

    def forward(self, hist: torch.Tensor, fcst: torch.Tensor = None) -> torch.Tensor:
        last = None
        
        # å¤„ç†å†å²æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.encoder is not None and hist.shape[-1] > 0:
            x = hist.permute(0, 2, 1)
            
            # æ£€æŸ¥è¾“å…¥é•¿åº¦ï¼Œå¦‚æœå¤ªçŸ­åˆ™ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚
            if x.shape[-1] < 5:  # å¦‚æœåºåˆ—é•¿åº¦å°äº5ï¼Œä½¿ç”¨çº¿æ€§å±‚æ›¿ä»£å·ç§¯
                # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ– + çº¿æ€§å±‚
                x_pooled = x.mean(dim=-1)  # (B, hist_dim)
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚æ¥æ›¿ä»£å·ç§¯ï¼Œè¾“å‡ºç»´åº¦è¦ä¸channels[-1]åŒ¹é…
                if not hasattr(self, 'fallback_linear'):
                    self.fallback_linear = nn.Linear(x.shape[1], self.channels[-1]).to(x.device)
                last = self.fallback_linear(x_pooled)  # (B, channels[-1])
            else:
                out = self.encoder(x)
                last = out[:, :, -1]

        # å¤„ç†é¢„æµ‹æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.use_fcst and fcst is not None and self.fcst_proj is not None:
            # ç®€åŒ–çš„é¢„æµ‹ç‰¹å¾å¤„ç†ï¼Œä¸MLæ¨¡å‹ä¿æŒä¸€è‡´
            f_flat = fcst.reshape(fcst.size(0), -1)
            f_proj = self.fcst_proj(f_flat)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” TCNè°ƒè¯•: fcstå½¢çŠ¶={fcst.shape}, f_flatå½¢çŠ¶={f_flat.shape}, f_projå½¢çŠ¶={f_proj.shape}")
            print(f"ğŸ” TCNè°ƒè¯•: f_projç»Ÿè®¡ - min={f_proj.min().item():.6f}, max={f_proj.max().item():.6f}, mean={f_proj.mean().item():.6f}")
            
            if last is not None:
                last = last + f_proj  # ç®€å•ç›¸åŠ èåˆ
                print(f"ğŸ” TCNè°ƒè¯•: èåˆålastç»Ÿè®¡ - min={last.min().item():.6f}, max={last.max().item():.6f}, mean={last.mean().item():.6f}")
            else:
                last = f_proj
                print(f"ğŸ” TCNè°ƒè¯•: ä½¿ç”¨é¢„æµ‹æ•°æ®ä½œä¸ºlast - min={last.min().item():.6f}, max={last.max().item():.6f}, mean={last.mean().item():.6f}")

        # å¦‚æœæ—¢æ²¡æœ‰å†å²æ•°æ®ä¹Ÿæ²¡æœ‰é¢„æµ‹æ•°æ®ï¼Œåˆ›å»ºé›¶å‘é‡
        if last is None:
            # åˆ›å»ºä¸€ä¸ªé›¶å‘é‡ä½œä¸ºé»˜è®¤è¾“å‡º
            batch_size = hist.size(0) if hist is not None else fcst.size(0)
            last = torch.zeros(batch_size, self.channels[-1]).to(hist.device if hist is not None else fcst.device)
            print(f"âš ï¸ è­¦å‘Š: TCNæ¨¡å‹æ²¡æœ‰æœ‰æ•ˆè¾“å…¥ï¼Œä½¿ç”¨é›¶å‘é‡ä½œä¸ºé»˜è®¤è¾“å‡º")

        # æ·»åŠ æ•°å€¼ç¨³å®šæ€§å¤„ç†
        output = self.head(last)
        
        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
        if torch.isnan(output).any():
            print(f"âš ï¸ è­¦å‘Š: TCNæ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œä½¿ç”¨é›¶å€¼æ›¿ä»£")
            output = torch.where(torch.isnan(output), torch.zeros_like(output), output)
        
        # ç¡®ä¿è¾“å‡ºä¸ºæ­£å€¼ï¼ˆå¤ªé˜³èƒ½å‘ç”µé‡ä¸èƒ½ä¸ºè´Ÿï¼‰
        output = torch.clamp(output, min=self.eps)
        
        return output
