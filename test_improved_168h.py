#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›çš„LSTMå’ŒGRUæ¨¡å‹ - 168å°æ—¶é¢„æµ‹
ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è§£å†³å‘¨æœŸæ€§é—®é¢˜
"""

import os
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from models.rnn_models import LSTM, GRU

def load_real_data():
    """åŠ è½½çœŸå®çš„Project1140æ•°æ®ï¼Œç›®æ ‡å˜é‡ä¸ºCapacity Factor"""
    print("ğŸ”§ åŠ è½½çœŸå®çš„Project1140æ•°æ®...")
    
    import pandas as pd
    
    data_path = "data/Project1140.csv"
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    df = pd.read_csv(data_path)
    print(f"âœ… åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    feature_cols = []
    
    # ç›®æ ‡å˜é‡ - Capacity Factor (0-100èŒƒå›´ï¼Œä¸æ ‡å‡†åŒ–)
    if 'Capacity Factor' in df.columns:
        feature_cols.append('Capacity Factor')
        print("âœ… ç›®æ ‡å˜é‡: Capacity Factor (èŒƒå›´0-100)")
    else:
        print("âŒ æœªæ‰¾åˆ°'Capacity Factor'åˆ—")
        return None
    
    # PVç‰¹å¾ - Electricity Generated
    if 'Electricity Generated' in df.columns:
        feature_cols.append('Electricity Generated')
        print("âœ… æ·»åŠ PVç‰¹å¾: Electricity Generated")
    
    # æ—¶é—´ç‰¹å¾ - è¿›è¡Œæ­£ä½™å¼¦è½¬æ¢
    time_features = []
    if 'Hour (Eastern Time, Daylight-Adjusted)' in df.columns:
        hour = df['Hour (Eastern Time, Daylight-Adjusted)'].values
        time_features.extend([
            np.sin(2 * np.pi * hour / 24),  # å°æ—¶çš„æ­£å¼¦ç¼–ç 
            np.cos(2 * np.pi * hour / 24)   # å°æ—¶çš„ä½™å¼¦ç¼–ç 
        ])
        print("âœ… æ·»åŠ æ—¶é—´ç‰¹å¾: Hour (æ­£ä½™å¼¦ç¼–ç )")
    
    if 'Month' in df.columns:
        month = df['Month'].values
        time_features.extend([
            np.sin(2 * np.pi * month / 12),  # æœˆä»½çš„æ­£å¼¦ç¼–ç 
            np.cos(2 * np.pi * month / 12)   # æœˆä»½çš„ä½™å¼¦ç¼–ç 
        ])
        print("âœ… æ·»åŠ æ—¶é—´ç‰¹å¾: Month (æ­£ä½™å¼¦ç¼–ç )")
    
    # å¤©æ°”ç‰¹å¾ - é€‰æ‹©ä¸»è¦çš„å¤©æ°”ç‰¹å¾
    weather_cols = [col for col in df.columns if any(x in col for x in [
        'temperature_2m', 'relative_humidity_2m', 'surface_pressure',
        'wind_speed_10m', 'global_tilted_irradiance', 'cloud_cover'
    ]) and not col.endswith('_pred')]
    
    if weather_cols:
        feature_cols.extend(weather_cols[:4])  # é€‰æ‹©å‰4ä¸ªå¤©æ°”ç‰¹å¾
        print(f"âœ… æ·»åŠ å¤©æ°”ç‰¹å¾: {weather_cols[:4]}")
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨
    available_cols = [col for col in feature_cols if col in df.columns]
    print(f"âœ… åŸºç¡€ç‰¹å¾åˆ—è¡¨ ({len(available_cols)}ä¸ª): {available_cols}")
    
    if len(available_cols) < 2:
        print("âŒ ç‰¹å¾åˆ—ä¸è¶³")
        return None
    
    # æå–åŸºç¡€æ•°æ®å¹¶å¤„ç†ç¼ºå¤±å€¼
    base_data = df[available_cols].fillna(method='ffill').fillna(method='bfill').values.astype(np.float32)
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾
    if time_features:
        time_array = np.column_stack(time_features).astype(np.float32)
        data = np.column_stack([base_data, time_array])
        print(f"âœ… æ·»åŠ æ—¶é—´ç¼–ç ç‰¹å¾: {len(time_features)}ä¸ª")
    else:
        data = base_data
    
    print(f"âœ… æœ€ç»ˆæ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"âœ… æ•°æ®èŒƒå›´: {data.min():.2f} - {data.max():.2f}")
    print(f"âœ… Capacity FactorèŒƒå›´: {data[:, 0].min():.2f} - {data[:, 0].max():.2f}")
    
    return data

def prepare_sequences(data, past_hours=168, future_hours=168):
    """å‡†å¤‡åºåˆ—æ•°æ®ï¼ŒCapacity Factorä½œä¸ºç›®æ ‡å˜é‡ï¼Œåªæ ‡å‡†åŒ–éç›®æ ‡ç‰¹å¾"""
    print("ğŸ”§ å‡†å¤‡åºåˆ—æ•°æ®...")
    
    # åˆ†ç¦»ç›®æ ‡å˜é‡å’Œç‰¹å¾
    capacity_factor = data[:, 0:1]  # Capacity Factor (ä¸æ ‡å‡†åŒ–)
    features = data[:, 1:]  # å…¶ä»–ç‰¹å¾ (éœ€è¦æ ‡å‡†åŒ–)
    
    # åªå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # é‡æ–°ç»„åˆæ•°æ®
    data_scaled = np.column_stack([capacity_factor, features_scaled])
    
    X, y = [], []
    for i in range(past_hours, len(data_scaled) - future_hours + 1):
        X.append(data_scaled[i-past_hours:i])  # è¾“å…¥åºåˆ—ï¼šæ‰€æœ‰ç‰¹å¾
        y.append(data_scaled[i:i+future_hours, 0])  # ç›®æ ‡åºåˆ—ï¼šCapacity Factor (ç¬¬ä¸€åˆ—ï¼Œæœªæ ‡å‡†åŒ–)
    
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    
    print(f"âœ… åºåˆ—æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    print(f"âœ… ç›®æ ‡å˜é‡èŒƒå›´ (Capacity Factor): {y.min():.2f} - {y.max():.2f}")
    
    # åˆ†å‰²æ•°æ®
    train_size = int(0.8 * len(X))
    val_size = int(0.1 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    
    print(f"âœ… è®­ç»ƒé›†: {X_train.shape}, éªŒè¯é›†: {X_val.shape}, æµ‹è¯•é›†: {X_test.shape}")
    
    return (X_train, y_train, X_val, y_val, X_test, y_test), scaler

def train_model(model, X_train, y_train, X_val, y_val, config):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{config['model']}æ¨¡å‹...")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {X_train.shape}, éªŒè¯æ•°æ®: {X_val.shape}")
    print(f"ğŸ¯ ç›®æ ‡å˜é‡èŒƒå›´: {y_train.min():.2f} - {y_train.max():.2f}")
    print(f"âš™ï¸ æ¨¡å‹é…ç½®: hidden_dim={config['hidden_dim']}, num_layers={config['num_layers']}, dropout={config['dropout']}")
    print(f"ğŸ“ˆ è®­ç»ƒå‚æ•°: epochs={config['epochs']}, batch_size={config['batch_size']}, lr={config['learning_rate']}")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.FloatTensor(y_val)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-4)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    print("=" * 60)
    print("ğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    print("=" * 60)
    
    # åˆ›å»ºepochè¿›åº¦æ¡
    epoch_pbar = tqdm(range(config['epochs']), desc="è®­ç»ƒè¿›åº¦", unit="epoch")
    
    for epoch in epoch_pbar:
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        
        # åˆ›å»ºbatchè¿›åº¦æ¡
        batch_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['epochs']}", 
                         leave=False, unit="batch")
        
        for batch_X, batch_y in batch_pbar:
            optimizer.zero_grad()
            
            # åˆ†ç¦»å†å²æ•°æ®å’Œé¢„æµ‹æ•°æ®
            hist_data = batch_X[:, :config['past_hours']]  # å†å²æ•°æ®
            fcst_data = batch_X[:, config['past_hours']:]  # é¢„æµ‹æ•°æ®
            
            # å‰å‘ä¼ æ’­
            outputs = model(hist_data, fcst_data)
            loss = criterion(outputs, batch_y)
            
            # æ£€æŸ¥NaN
            if torch.isnan(loss):
                tqdm.write(f"âŒ Epoch {epoch+1} å‡ºç°NaNæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            
            train_loss += loss.item()
            
            # æ›´æ–°batchè¿›åº¦æ¡
            batch_pbar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Avg Loss': f'{train_loss/(batch_pbar.n+1):.6f}'
            })
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            hist_val = X_val_tensor[:, :config['past_hours']]
            fcst_val = X_val_tensor[:, config['past_hours']:]
            val_outputs = model(hist_val, fcst_val)
            val_loss = criterion(val_outputs, y_val_tensor).item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        val_losses.append(val_loss)
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss - config['min_delta']:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        # æ›´æ–°epochè¿›åº¦æ¡
        current_lr = optimizer.param_groups[0]['lr']
        epoch_pbar.set_postfix({
            'Train Loss': f'{avg_train_loss:.6f}',
            'Val Loss': f'{val_loss:.6f}',
            'LR': f'{current_lr:.2e}',
            'Patience': f'{patience_counter}/{config["patience"]}'
        })
        
        # è¯¦ç»†è®­ç»ƒè¿›ç¨‹è¾“å‡º
        if epoch % 5 == 0 or epoch == config['epochs'] - 1:
            tqdm.write(f"Epoch {epoch+1:3d}/{config['epochs']:3d} | "
                      f"Train Loss: {avg_train_loss:.6f} | "
                      f"Val Loss: {val_loss:.6f} | "
                      f"LR: {current_lr:.2e} | "
                      f"Patience: {patience_counter}/{config['patience']}")
        
        if patience_counter >= config['patience']:
            tqdm.write(f"ğŸ›‘ æ—©åœäºç¬¬ {epoch+1} è½®")
            break
    
    # å…³é—­è¿›åº¦æ¡
    epoch_pbar.close()
    
    print("=" * 60)
    print(f"âœ… {config['model']}è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
    print("=" * 60)
    return train_losses, val_losses

def generate_predictions(model, X_test, y_test, config, model_name):
    """ç”Ÿæˆé¢„æµ‹å¹¶å¯è§†åŒ–"""
    print(f"ğŸ¨ ç”Ÿæˆ{model_name}çš„168å°æ—¶é¢„æµ‹...")
    
    # é€‰æ‹©å‡ ä¸ªæµ‹è¯•æ ·æœ¬
    n_samples = min(3, len(X_test))
    sample_indices = np.random.choice(len(X_test), n_samples, replace=False)
    
    model.eval()
    with torch.no_grad():
        predictions = []
        ground_truths = []
        
        for idx in sample_indices:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            X_sample = torch.FloatTensor(X_test[idx:idx+1])
            hist_data = X_sample[:, :config['past_hours']]
            fcst_data = X_sample[:, config['past_hours']:]
            
            # ç”Ÿæˆé¢„æµ‹
            pred = model(hist_data, fcst_data)
            pred_np = pred.cpu().numpy()[0]
            
            predictions.append(pred_np)
            ground_truths.append(y_test[idx])
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(n_samples, 1, figsize=(15, 5*n_samples))
    if n_samples == 1:
        axes = [axes]
    
    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
        hours = range(1, len(pred) + 1)
        
        axes[i].plot(hours, gt, 'b-', label='Ground Truth', linewidth=2, alpha=0.8)
        axes[i].plot(hours, pred, 'r--', label=f'Improved {model_name} Prediction', linewidth=2, alpha=0.8)
        
        axes[i].set_xlabel('Hours Ahead')
        axes[i].set_ylabel('PV Power')
        axes[i].set_title(f'Sample {i+1}: 168-Hour PV Power Prediction - {model_name}')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæŒ‡æ ‡
        mse = np.mean((pred - gt) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(pred - gt))
        
        axes[i].text(0.02, 0.98, f'RMSE: {rmse:.3f}\nMAE: {mae:.3f}', 
                    transform=axes[i].transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(f'improved_{model_name.lower()}_168h_prediction.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… {model_name}é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    return predictions, ground_truths

def plot_168h_comparison(models, scaler):
    """ç»˜åˆ¶168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾"""
    print("ğŸ“Š ç»˜åˆ¶168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾...")
    
    fig, axes = plt.subplots(2, 1, figsize=(15, 10))
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    sample_idx = 0
    lstm_pred = models['LSTM']['predictions'][sample_idx]
    gru_pred = models['GRU']['predictions'][sample_idx]
    y_true = models['LSTM']['ground_truths'][sample_idx]
    
    # åæ ‡å‡†åŒ–
    def denormalize_single(pred, scaler):
        temp_array = np.zeros((len(pred), scaler.n_features_in_))
        temp_array[:, 0] = pred
        return scaler.inverse_transform(temp_array)[:, 0]
    
    lstm_pred_denorm = denormalize_single(lstm_pred, scaler)
    gru_pred_denorm = denormalize_single(gru_pred, scaler)
    y_true_denorm = denormalize_single(y_true, scaler)
    
    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
    time_steps = range(168)
    axes[0].plot(time_steps, y_true_denorm, 'b-', label='çœŸå®å€¼ (Capacity Factor)', linewidth=2)
    axes[0].plot(time_steps, lstm_pred_denorm, 'r--', label='LSTMé¢„æµ‹', linewidth=2)
    axes[0].plot(time_steps, gru_pred_denorm, 'g--', label='GRUé¢„æµ‹', linewidth=2)
    
    axes[0].set_title('LSTM vs GRU é¢„æµ‹å¯¹æ¯” (å‰168å°æ—¶) - Capacity Factor', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('æ—¶é—´ (å°æ—¶)')
    axes[0].set_ylabel('Capacity Factor')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶è¯¯å·®å¯¹æ¯”
    lstm_error = np.abs(lstm_pred_denorm - y_true_denorm)
    gru_error = np.abs(gru_pred_denorm - y_true_denorm)
    
    axes[1].plot(time_steps, lstm_error, 'r-', label='LSTMè¯¯å·®', linewidth=2)
    axes[1].plot(time_steps, gru_error, 'g-', label='GRUè¯¯å·®', linewidth=2)
    
    axes[1].set_title('é¢„æµ‹è¯¯å·®å¯¹æ¯” (Capacity Factor)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('æ—¶é—´ (å°æ—¶)')
    axes[1].set_ylabel('ç»å¯¹è¯¯å·®')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('improved_lstm_gru_comparison_168h.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… 168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º: improved_lstm_gru_comparison_168h.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•æ”¹è¿›çš„RNNæ¨¡å‹ - 168å°æ—¶é¢„æµ‹")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½® - å‚è€ƒcomplexity lowè®¾ç½®
    config = {
        'model': 'LSTM',
        'hidden_dim': 32,        # low: 32
        'num_layers': 6,         # low: 6
        'dropout': 0.1,          # low: 0.1
        'd_model': 64,           # low: 64
        'num_heads': 4,          # low: 4
        'future_hours': 168,     # 168å°æ—¶é¢„æµ‹
        'past_hours': 168,       # 168å°æ—¶å†å²
        'use_forecast': True,
        'epochs': 50,            # low: 50
        'batch_size': 64,        # low: 64
        'learning_rate': 0.001,  # low: 0.001
        'patience': 10,
        'min_delta': 0.001
    }
    
    # åŠ è½½çœŸå®æ•°æ®
    data = load_real_data()
    if data is None:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # å‡†å¤‡åºåˆ—æ•°æ®
    data_splits, scaler = prepare_sequences(data, config['past_hours'], config['future_hours'])
    X_train, y_train, X_val, y_val, X_test, y_test = data_splits
    
    # æµ‹è¯•LSTMå’ŒGRU
    models = {}
    for model_type in ['LSTM', 'GRU']:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯• {model_type} æ¨¡å‹")
        print(f"{'='*50}")
        
        config['model'] = model_type
        hist_dim = X_train.shape[-1]
        fcst_dim = X_train.shape[-1] if config.get('use_forecast', False) else 0
        
        if model_type == 'LSTM':
            model = LSTM(hist_dim, fcst_dim, config)
        else:
            model = GRU(hist_dim, fcst_dim, config)
        
        print(f"âœ… {model_type}å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # è®­ç»ƒæ¨¡å‹
        train_losses, val_losses = train_model(model, X_train, y_train, X_val, y_val, config)
        
        # ç”Ÿæˆé¢„æµ‹
        predictions, ground_truths = generate_predictions(model, X_test, y_test, config, model_type)
        
        models[model_type] = {
            'model': model,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'predictions': predictions,
            'ground_truths': ground_truths
        }
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(models['LSTM']['train_losses'], label='LSTM Train', color='blue')
    plt.plot(models['LSTM']['val_losses'], label='LSTM Val', color='blue', linestyle='--')
    plt.plot(models['GRU']['train_losses'], label='GRU Train', color='red')
    plt.plot(models['GRU']['val_losses'], label='GRU Val', color='red', linestyle='--')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Progress Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # é¢„æµ‹ç²¾åº¦å¯¹æ¯” (åæ ‡å‡†åŒ–å)
    plt.subplot(1, 3, 2)
    
    # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœå’ŒçœŸå®å€¼
    def denormalize_predictions(predictions, ground_truths, scaler):
        denorm_preds = []
        denorm_gts = []
        for pred, gt in zip(predictions, ground_truths):
            # åˆ›å»ºä¸´æ—¶æ•°ç»„è¿›è¡Œåæ ‡å‡†åŒ–
            temp_pred = np.zeros((len(pred), scaler.n_features_in_))
            temp_pred[:, 0] = pred
            temp_gt = np.zeros((len(gt), scaler.n_features_in_))
            temp_gt[:, 0] = gt
            
            denorm_pred = scaler.inverse_transform(temp_pred)[:, 0]
            denorm_gt = scaler.inverse_transform(temp_gt)[:, 0]
            
            denorm_preds.append(denorm_pred)
            denorm_gts.append(denorm_gt)
        return denorm_preds, denorm_gts
    
    lstm_preds_denorm, lstm_gts_denorm = denormalize_predictions(models['LSTM']['predictions'], models['LSTM']['ground_truths'], scaler)
    gru_preds_denorm, gru_gts_denorm = denormalize_predictions(models['GRU']['predictions'], models['GRU']['ground_truths'], scaler)
    
    lstm_rmse = [np.sqrt(np.mean((pred - gt) ** 2)) for pred, gt in zip(lstm_preds_denorm, lstm_gts_denorm)]
    gru_rmse = [np.sqrt(np.mean((pred - gt) ** 2)) for pred, gt in zip(gru_preds_denorm, gru_gts_denorm)]
    
    plt.bar(['LSTM', 'GRU'], [np.mean(lstm_rmse), np.mean(gru_rmse)], color=['blue', 'red'], alpha=0.7)
    plt.ylabel('Average RMSE (Capacity Factor)')
    plt.title('Prediction Accuracy Comparison')
    plt.grid(True, alpha=0.3)
    
    # å‚æ•°æ•°é‡å¯¹æ¯”
    plt.subplot(1, 3, 3)
    lstm_params = sum(p.numel() for p in models['LSTM']['model'].parameters())
    gru_params = sum(p.numel() for p in models['GRU']['model'].parameters())
    
    plt.bar(['LSTM', 'GRU'], [lstm_params, gru_params], color=['blue', 'red'], alpha=0.7)
    plt.ylabel('Number of Parameters')
    plt.title('Model Complexity Comparison')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('improved_rnn_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç»˜åˆ¶168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾
    print("\nğŸ“Š ç»˜åˆ¶168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾...")
    plot_168h_comparison(models, scaler)
    
    print("\nğŸ¯ æ”¹è¿›æ•ˆæœæ€»ç»“:")
    print("   - æ·»åŠ äº†å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æ¨¡å‹å…³æ³¨é‡è¦çš„æ—¶é—´æ­¥")
    print("   - ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œæ”¹å–„æ¢¯åº¦æµå’Œè®­ç»ƒç¨³å®šæ€§")
    print("   - ç»Ÿä¸€äº†LSTMå’ŒGRUçš„æ¶æ„é…ç½®")
    print("   - ä¸“é—¨é’ˆå¯¹168å°æ—¶é•¿æœŸé¢„æµ‹è¿›è¡Œäº†ä¼˜åŒ–")
    print("   - æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸é—®é¢˜")
    print("   - ä½¿ç”¨çœŸå®Project1140æ•°æ®è®­ç»ƒï¼Œç›®æ ‡å˜é‡ä¸ºCapacity Factor")

if __name__ == "__main__":
    main()
