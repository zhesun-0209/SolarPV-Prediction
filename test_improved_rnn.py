#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›çš„LSTMå’ŒGRUæ¨¡å‹
éªŒè¯æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®è¿æ¥çš„æ•ˆæœ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from models.rnn_models import LSTM, GRU, MultiHeadTemporalAttention

def test_multihead_attention():
    """æµ‹è¯•å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶"""
    print("ğŸ” æµ‹è¯•å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶...")
    
    batch_size, seq_len, hidden_dim = 2, 10, 64
    num_heads = 8
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    rnn_outputs = torch.randn(batch_size, seq_len, hidden_dim)
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    attention = MultiHeadTemporalAttention(hidden_dim, num_heads=num_heads)
    
    # å‰å‘ä¼ æ’­
    attended_output, attention_weights = attention(rnn_outputs)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {rnn_outputs.shape}")
    print(f"âœ… æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attended_output.shape}")
    print(f"âœ… æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"âœ… æ³¨æ„åŠ›æƒé‡å’Œ: {attention_weights.sum(dim=-1).mean()}")
    
    return True

def test_improved_models():
    """æµ‹è¯•æ”¹è¿›çš„LSTMå’ŒGRUæ¨¡å‹"""
    print("\nğŸ” æµ‹è¯•æ”¹è¿›çš„LSTMå’ŒGRUæ¨¡å‹...")
    
    # é…ç½®å‚æ•°
    config = {
        'hidden_dim': 64,
        'num_layers': 2,
        'dropout': 0.1,
        'future_hours': 168,
        'use_forecast': True
    }
    
    # åˆ›å»ºæ¨¡å‹
    lstm_model = LSTM(hist_dim=10, fcst_dim=5, config=config)
    gru_model = GRU(hist_dim=10, fcst_dim=5, config=config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    hist_seq_len = 24
    fcst_seq_len = 168
    
    hist_data = torch.randn(batch_size, hist_seq_len, 10)
    fcst_data = torch.randn(batch_size, fcst_seq_len, 5)
    
    # æµ‹è¯•LSTM
    print("\nğŸ“Š æµ‹è¯•LSTMæ¨¡å‹:")
    lstm_output = lstm_model(hist_data, fcst_data)
    print(f"âœ… LSTMè¾“å‡ºå½¢çŠ¶: {lstm_output.shape}")
    print(f"âœ… LSTMè¾“å‡ºèŒƒå›´: [{lstm_output.min():.2f}, {lstm_output.max():.2f}]")
    
    # æµ‹è¯•GRU
    print("\nğŸ“Š æµ‹è¯•GRUæ¨¡å‹:")
    gru_output = gru_model(hist_data, fcst_data)
    print(f"âœ… GRUè¾“å‡ºå½¢çŠ¶: {gru_output.shape}")
    print(f"âœ… GRUè¾“å‡ºèŒƒå›´: [{gru_output.min():.2f}, {gru_output.max():.2f}]")
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ•°é‡
    lstm_params = sum(p.numel() for p in lstm_model.parameters())
    gru_params = sum(p.numel() for p in gru_model.parameters())
    
    print(f"\nğŸ“ˆ æ¨¡å‹å‚æ•°æ•°é‡:")
    print(f"âœ… LSTMå‚æ•°: {lstm_params:,}")
    print(f"âœ… GRUå‚æ•°: {gru_params:,}")
    
    return lstm_model, gru_model

def visualize_attention_weights(model, hist_data, fcst_data, model_name="LSTM"):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    print(f"\nğŸ¨ å¯è§†åŒ–{model_name}æ³¨æ„åŠ›æƒé‡...")
    
    model.eval()
    with torch.no_grad():
        # è·å–æ³¨æ„åŠ›æƒé‡
        if hasattr(model, 'temporal_attention'):
            # æ‰‹åŠ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡
            seqs = []
            if model.hist_proj is not None:
                h_proj = model.hist_proj(hist_data)
                seqs.append(h_proj)
            if model.fcst_proj is not None:
                f_proj = model.fcst_proj(fcst_data)
                seqs.append(f_proj)
            
            seq = torch.cat(seqs, dim=1)
            if model_name == "LSTM":
                rnn_outputs, _ = model.lstm(seq)
            else:
                rnn_outputs, _ = model.gru(seq)
            
            _, attention_weights = model.temporal_attention(rnn_outputs)
            
            # ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾
            plt.figure(figsize=(12, 6))
            # å¯¹äºå¤šå¤´æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡
            attention_vis = attention_weights[0].cpu().numpy()  # (seq_len, seq_len)
            plt.imshow(attention_vis, aspect='auto', cmap='Blues')
            plt.colorbar(label='Attention Weight')
            plt.title(f'{model_name} Multi-Head Temporal Attention Weights (Sample 1)')
            plt.xlabel('Key Time Steps')
            plt.ylabel('Query Time Steps')
            plt.tight_layout()
            plt.savefig(f'{model_name.lower()}_attention_weights.png', dpi=150, bbox_inches='tight')
            plt.show()
            
            print(f"âœ… æ³¨æ„åŠ›æƒé‡å·²ä¿å­˜ä¸º: {model_name.lower()}_attention_weights.png")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ”¹è¿›çš„RNNæ¨¡å‹")
    print("=" * 50)
    
    # æµ‹è¯•å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
    test_multihead_attention()
    
    # æµ‹è¯•æ”¹è¿›çš„æ¨¡å‹
    lstm_model, gru_model = test_improved_models()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ç”¨äºå¯è§†åŒ–
    config = {
        'hidden_dim': 64,
        'num_layers': 2,
        'dropout': 0.1,
        'future_hours': 168,
        'use_forecast': True
    }
    
    hist_data = torch.randn(2, 24, 10)
    fcst_data = torch.randn(2, 168, 5)
    
    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    visualize_attention_weights(lstm_model, hist_data, fcst_data, "LSTM")
    visualize_attention_weights(gru_model, hist_data, fcst_data, "GRU")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("ğŸ¯ æ”¹è¿›è¦ç‚¹:")
    print("   - ç»Ÿä¸€äº†LSTMå’ŒGRUçš„æ¶æ„é…ç½®")
    print("   - æ·»åŠ äº†æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶è§£å†³å‘¨æœŸæ€§é—®é¢˜")
    print("   - ä½¿ç”¨æ®‹å·®è¿æ¥æ”¹å–„æ¢¯åº¦æµ")
    print("   - ä¿æŒäº†ç›¸åŒçš„è¾“å‡ºæ ¼å¼å’ŒèŒƒå›´")

if __name__ == "__main__":
    main()
