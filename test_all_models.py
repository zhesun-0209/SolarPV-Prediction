#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•æ‰€æœ‰DLå’ŒMLæ¨¡å‹
"""

import os
import sys
import yaml
import pandas as pd
import numpy as np
from pathlib import Path
import traceback
import subprocess
from datetime import datetime

def test_all_model_types():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å‹ç±»å‹"""
    print("ğŸ” æµ‹è¯•æ‰€æœ‰æ¨¡å‹ç±»å‹")
    print("=" * 60)
    
    # æ¨¡å‹åˆ—è¡¨
    models = {
        'ML': ['Linear', 'RF', 'XGB', 'LGBM'],
        'DL': ['LSTM', 'GRU', 'TCN', 'Transformer']
    }
    
    results = {}
    
    for model_type, model_list in models.items():
        print(f"\nğŸ“‹ æµ‹è¯•{model_type}æ¨¡å‹:")
        results[model_type] = {}
        
        for model in model_list:
            print(f"\n  ğŸ”§ æµ‹è¯• {model} æ¨¡å‹:")
            
            # åˆ›å»ºæµ‹è¯•é…ç½®
            config = create_test_config(model, model_type)
            
            # æµ‹è¯•æ¨¡å‹è®­ç»ƒ
            success = test_single_model(model, config)
            results[model_type][model] = success
            
            if success:
                print(f"    âœ… {model} æµ‹è¯•é€šè¿‡")
            else:
                print(f"    âŒ {model} æµ‹è¯•å¤±è´¥")
    
    return results

def create_test_config(model, model_type):
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    config = {
        'model': model,
        'data_path': 'data/Project1140.csv',
        'save_dir': f'temp_test_results/{model}',
        'past_hours': 24,
        'future_hours': 24,
        'use_pv': True,
        'use_hist_weather': False,
        'use_forecast': False,
        'weather_category': 'none',
        'use_time_encoding': False,
        'train_ratio': 0.8,
        'val_ratio': 0.1,
        'epochs': 2,  # å‡å°‘epochsç”¨äºå¿«é€Ÿæµ‹è¯•
        'batch_size': 32,
        'learning_rate': 0.001,
        'loss_type': 'mse'
    }
    
    # æ·»åŠ æ¨¡å‹ç‰¹å®šå‚æ•°ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç»“æ„ï¼‰
    if model_type == 'ML':
        if model == 'Linear':
            config['model_params'] = {
                'ml_low': {},
                'ml_high': {}
            }
        elif model == 'RF':
            config['model_params'] = {
                'ml_low': {
                    'n_estimators': 10,
                    'max_depth': 3,
                    'random_state': 42
                },
                'ml_high': {
                    'n_estimators': 20,
                    'max_depth': 5,
                    'random_state': 42
                }
            }
        elif model == 'XGB':
            config['model_params'] = {
                'ml_low': {
                    'n_estimators': 10,
                    'max_depth': 3,
                    'learning_rate': 0.1,
                    'verbosity': 0
                },
                'ml_high': {
                    'n_estimators': 20,
                    'max_depth': 5,
                    'learning_rate': 0.05,
                    'verbosity': 0
                }
            }
        elif model == 'LGBM':
            config['model_params'] = {
                'ml_low': {
                    'n_estimators': 10,
                    'max_depth': 3,
                    'learning_rate': 0.1,
                    'random_state': 42
                },
                'ml_high': {
                    'n_estimators': 20,
                    'max_depth': 5,
                    'learning_rate': 0.05,
                    'random_state': 42
                }
            }
    else:  # DL models
        config['model_params'] = {
            'low': {
                'd_model': 32,
                'num_heads': 2,
                'num_layers': 2,
                'hidden_dim': 16,
                'dropout': 0.1,
                'tcn_channels': [16, 32],
                'kernel_size': 3
            },
            'high': {
                'd_model': 64,
                'num_heads': 4,
                'num_layers': 4,
                'hidden_dim': 32,
                'dropout': 0.2,
                'tcn_channels': [32, 64],
                'kernel_size': 5
            }
        }
        # æ·»åŠ train_params
        config['train_params'] = {
            'batch_size': 32,
            'learning_rate': 0.001,
            'loss_type': 'mse'
        }
    
    return config

def test_single_model(model, config):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆç¡®ä¿æ•°æ®é‡è¶³å¤Ÿï¼‰
        future_hours = config.get('future_hours', 24)
        past_hours = config.get('past_hours', 24)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºä¸åŒç»´åº¦çš„æ•°æ®
        if config['model'] in ['Linear', 'RF', 'XGB', 'LGBM']:
            # MLæ¨¡å‹ä½¿ç”¨2Dæ•°æ®
            X_train = np.random.rand(100, 5)
            Xf_train = np.random.rand(100, 3)
            y_train = np.random.rand(100, future_hours)
            Xh_test = np.random.rand(30, 5)
            Xf_test = np.random.rand(30, 3)
            y_test = np.random.rand(30, future_hours)
        else:
            # DLæ¨¡å‹ä½¿ç”¨3Dæ•°æ® (samples, sequence_length, features)
            X_train = np.random.rand(100, past_hours, 5)
            Xf_train = np.random.rand(100, past_hours, 3)
            y_train = np.random.rand(100, future_hours)  # è¾“å‡ºä¿æŒ2D
            Xh_test = np.random.rand(30, past_hours, 5)
            Xf_test = np.random.rand(30, past_hours, 3)
            y_test = np.random.rand(30, future_hours)  # è¾“å‡ºä¿æŒ2D
            
        dates_test = [f"2024-01-01 {i:02d}:00:00" for i in range(30)]
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è®­ç»ƒå‡½æ•°
        if config['model'] in ['Linear', 'RF', 'XGB', 'LGBM']:
            from train.train_ml import train_ml_model
            model_obj, metrics = train_ml_model(
                config=config,
                Xh_train=X_train,
                Xf_train=Xf_train,
                y_train=y_train,
                Xh_test=Xh_test,
                Xf_test=Xf_test,
                y_test=y_test,
                dates_test=dates_test
            )
        else:  # DL models - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°ç»“æ„
            from train.train_dl import train_dl_model
            
            # åˆ›å»ºDLæ¨¡å‹æ‰€éœ€çš„æ•°æ®ç»“æ„
            train_data = (X_train, Xf_train, y_train, np.zeros(100), None)
            val_data = (X_train[:20], Xf_train[:20], y_train[:20], np.zeros(20), None)
            test_data = (Xh_test, Xf_test, y_test, np.zeros(30), dates_test)
            scalers = (None, None, None)
            
            model_obj, metrics = train_dl_model(
                config=config,
                train_data=train_data,
                val_data=val_data,
                test_data=test_data,
                scalers=scalers
            )
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        key_metrics = ['mae', 'rmse', 'r2', 'mape']
        missing_metrics = [m for m in key_metrics if m not in metrics]
        
        if missing_metrics:
            print(f"    âš ï¸  ç¼ºå°‘æŒ‡æ ‡: {missing_metrics}")
            return False
        
        print(f"    ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {[(k, f'{v:.4f}') for k, v in metrics.items() if k in key_metrics]}")
        return True
        
    except Exception as e:
        print(f"    âŒ é”™è¯¯: {e}")
        traceback.print_exc()
        return False

def test_result_saving_all_models():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å‹çš„ç»“æœä¿å­˜"""
    print("\nğŸ” æµ‹è¯•ç»“æœä¿å­˜åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from eval.eval_utils import save_results
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_metrics = {
            'mae': 0.123,
            'rmse': 0.234,
            'r2': 0.567,
            'mape': 12.34,
            'train_time_sec': 5.67,
            'inference_time_sec': 0.12,
            'param_count': 100,
            'samples_count': 1000,
            'predictions': np.random.rand(20, 1),
            'y_true': np.random.rand(20, 1)
        }
        
        test_dates = [f"2024-01-01 {i:02d}:00:00" for i in range(20)]
        test_y_true = np.random.rand(20)
        test_Xh = np.random.rand(20, 5)
        test_Xf = np.random.rand(20, 3)
        
        # æµ‹è¯•ä¿å­˜ç›®å½•
        test_save_dir = Path("debug_test_results_all")
        test_save_dir.mkdir(exist_ok=True)
        
        test_config = {
            'save_dir': str(test_save_dir),
            'model': 'Linear',
            'plot_days': 7,
            'past_hours': 24,
            'future_hours': 24
        }
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        class MockModel:
            def predict(self, X):
                return np.random.rand(X.shape[0], 1)
        
        mock_model = MockModel()
        
        # ä¿å­˜ç»“æœ
        save_results(
            model=mock_model,
            metrics=test_metrics,
            dates=test_dates,
            y_true=test_y_true,
            Xh_test=test_Xh,
            Xf_test=test_Xf,
            config=test_config
        )
        
        print("âœ… ç»“æœä¿å­˜æˆåŠŸ")
        
        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        result_files = list(test_save_dir.glob("*"))
        print(f"   ä¿å­˜çš„æ–‡ä»¶: {[f.name for f in result_files]}")
        
        # æ£€æŸ¥Excelæ–‡ä»¶
        excel_files = list(test_save_dir.glob("*.xlsx"))
        if excel_files:
            excel_file = excel_files[0]
            df = pd.read_excel(excel_file)
            print(f"   Excelæ–‡ä»¶å†…å®¹å½¢çŠ¶: {df.shape}")
            print(f"   Excelåˆ—å: {list(df.columns)}")
            
            # éªŒè¯å…³é”®æŒ‡æ ‡
            key_metrics = ['mae', 'rmse', 'r2', 'mape', 'train_time_sec']
            for metric in key_metrics:
                if metric in df.columns:
                    saved_value = df[metric].iloc[0]
                    original_value = test_metrics[metric]
                    print(f"   âœ… {metric}: æœŸæœ›{original_value}, å®é™…{saved_value}")
                else:
                    print(f"   âŒ ç¼ºå°‘åˆ—: {metric}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        import shutil
        shutil.rmtree(test_save_dir)
        print("âœ… æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç»“æœä¿å­˜å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_full_experiment_pipeline():
    """æµ‹è¯•å®Œæ•´å®éªŒæµç¨‹"""
    print("\nğŸ” æµ‹è¯•å®Œæ•´å®éªŒæµç¨‹")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {'model': 'Linear', 'complexity': 'low'},
        {'model': 'RF', 'complexity': 'low'},
        {'model': 'LSTM', 'complexity': 'low'}
    ]
    
    results = {}
    
    for config_info in test_configs:
        model = config_info['model']
        complexity = config_info['complexity']
        
        print(f"\n  ğŸ”§ æµ‹è¯• {model}_{complexity} å®Œæ•´æµç¨‹:")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config = create_test_config(model, 'ML' if model in ['Linear', 'RF', 'XGB', 'LGBM'] else 'DL')
        config['model_complexity'] = complexity
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_dir = Path("temp_test_configs")
        config_dir.mkdir(exist_ok=True)
        config_file = config_dir / f"{model}_{complexity}_test.yaml"
        
        with open(config_file, 'w') as f:
            yaml.dump(config, f)
        
        try:
            # è¿è¡Œå®éªŒ
            cmd = ['python', 'main.py', '--config', str(config_file)]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                print(f"    âœ… {model}_{complexity} å®éªŒè¿è¡ŒæˆåŠŸ")
                
                # æ£€æŸ¥ç»“æœæ–‡ä»¶
                save_dir = Path(config['save_dir'])
                if save_dir.exists():
                    result_files = list(save_dir.glob("*"))
                    print(f"    ğŸ“ ç»“æœæ–‡ä»¶: {[f.name for f in result_files]}")
                    
                    # æ£€æŸ¥Excelæ–‡ä»¶
                    excel_files = list(save_dir.glob("*.xlsx"))
                    if excel_files:
                        df = pd.read_excel(excel_files[0])
                        print(f"    ğŸ“Š Excelå†…å®¹: {df.shape}, åˆ—: {list(df.columns)}")
                        
                        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
                        key_metrics = ['mae', 'rmse', 'r2', 'mape']
                        for metric in key_metrics:
                            if metric in df.columns:
                                value = df[metric].iloc[0]
                                print(f"    âœ… {metric}: {value}")
                            else:
                                print(f"    âŒ ç¼ºå°‘æŒ‡æ ‡: {metric}")
                    
                    results[f"{model}_{complexity}"] = True
                else:
                    print(f"    âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {save_dir}")
                    results[f"{model}_{complexity}"] = False
            else:
                print(f"    âŒ {model}_{complexity} å®éªŒè¿è¡Œå¤±è´¥")
                print(f"    é”™è¯¯è¾“å‡º: {result.stderr[-500:]}")
                results[f"{model}_{complexity}"] = False
                
        except subprocess.TimeoutExpired:
            print(f"    â° {model}_{complexity} å®éªŒè¶…æ—¶")
            results[f"{model}_{complexity}"] = False
        except Exception as e:
            print(f"    âŒ {model}_{complexity} å®éªŒå¼‚å¸¸: {e}")
            results[f"{model}_{complexity}"] = False
        
        # æ¸…ç†é…ç½®æ–‡ä»¶
        config_file.unlink()
    
    # æ¸…ç†é…ç½®ç›®å½•
    config_dir.rmdir()
    
    return results

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹æµ‹è¯•")
    print("=" * 80)
    
    test_results = {}
    
    # æµ‹è¯•1: æ‰€æœ‰æ¨¡å‹ç±»å‹
    test_results['model_types'] = test_all_model_types()
    
    # æµ‹è¯•2: ç»“æœä¿å­˜åŠŸèƒ½
    test_results['result_saving'] = test_result_saving_all_models()
    
    # æµ‹è¯•3: å®Œæ•´å®éªŒæµç¨‹
    test_results['full_pipeline'] = test_full_experiment_pipeline()
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ¯ å…¨é¢æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    # æ¨¡å‹ç±»å‹æµ‹è¯•ç»“æœ
    print("\nğŸ“‹ æ¨¡å‹ç±»å‹æµ‹è¯•ç»“æœ:")
    for model_type, models in test_results['model_types'].items():
        print(f"  {model_type}æ¨¡å‹:")
        for model, success in models.items():
            status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
            print(f"    {model}: {status}")
    
    # ç»“æœä¿å­˜æµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š ç»“æœä¿å­˜æµ‹è¯•: {'âœ… é€šè¿‡' if test_results['result_saving'] else 'âŒ å¤±è´¥'}")
    
    # å®Œæ•´æµç¨‹æµ‹è¯•ç»“æœ
    print(f"\nğŸ”„ å®Œæ•´æµç¨‹æµ‹è¯•ç»“æœ:")
    for test_name, success in test_results['full_pipeline'].items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    # æ€»ä½“ç»Ÿè®¡
    total_tests = sum(len(models) for models in test_results['model_types'].values())
    passed_tests = sum(sum(models.values()) for models in test_results['model_types'].values())
    
    if test_results['result_saving']:
        passed_tests += 1
    total_tests += 1
    
    passed_tests += sum(test_results['full_pipeline'].values())
    total_tests += len(test_results['full_pipeline'])
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œå®éªŒ")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
    
    return test_results

if __name__ == "__main__":
    main()
