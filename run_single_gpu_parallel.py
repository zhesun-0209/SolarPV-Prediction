#!/usr/bin/env python3
"""
å•GPUå¹¶è¡Œå®éªŒè„šæœ¬
åœ¨å•å—GPUä¸Šæ™ºèƒ½è°ƒåº¦å¤šä¸ªå®éªŒ
"""

import os
import sys
import time
import yaml
import threading
import queue
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import torch
import subprocess

class SingleGPUParallelScheduler:
    """å•GPUå¹¶è¡Œè°ƒåº¦å™¨"""
    
    def __init__(self, max_parallel=2, gpu_memory_limit_mb=12000):
        self.max_parallel = max_parallel
        self.gpu_memory_limit_mb = gpu_memory_limit_mb
        self.experiment_queue = queue.Queue()
        self.running_experiments = {}
        self.completed_experiments = []
        self.failed_experiments = []
        self.lock = threading.Lock()
        
        # æ¨¡å‹å†…å­˜éœ€æ±‚ä¼°ç®—ï¼ˆMBï¼‰
        self.model_memory_requirements = {
            'RF': 500,
            'XGB': 1000,
            'LGBM': 1000,
            'TCN_low': 1500,
            'LSTM_low': 2000,
            'GRU_low': 2000,
            'Transformer_low': 2500,
            'TCN_high': 3000,
            'LSTM_high': 4000,
            'GRU_high': 4000,
            'Transformer_high': 6000
        }
        
        # å®éªŒä¼˜å…ˆçº§
        self.experiment_priorities = {
            'RF': 1,
            'XGB': 2,
            'LGBM': 3,
            'TCN_low': 4,
            'LSTM_low': 5,
            'GRU_low': 6,
            'Transformer_low': 7,
            'TCN_high': 8,
            'LSTM_high': 9,
            'GRU_high': 10,
            'Transformer_high': 11
        }
    
    def add_experiment(self, config_file, data_file, project_id):
        """æ·»åŠ å®éªŒåˆ°é˜Ÿåˆ—"""
        config_name = os.path.basename(config_file).replace('.yaml', '')
        model_name = config_name.split('_')[0]
        complexity = config_name.split('_')[1]
        
        priority_key = model_name if model_name in self.experiment_priorities else f"{model_name}_{complexity}"
        priority = self.experiment_priorities.get(priority_key, 999)
        memory_required = self.model_memory_requirements.get(f"{model_name}_{complexity}", 2000)
        
        experiment = {
            'config_file': config_file,
            'data_file': data_file,
            'project_id': project_id,
            'config_name': config_name,
            'model_name': model_name,
            'complexity': complexity,
            'priority': priority,
            'memory_required': memory_required,
            'status': 'queued',
            'start_time': None,
            'end_time': None
        }
        
        self.experiment_queue.put(experiment)
        return experiment
    
    def can_start_experiment(self, experiment):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¯åŠ¨å®éªŒ"""
        if len(self.running_experiments) >= self.max_parallel:
            return False
        
        # æ£€æŸ¥æ€»å†…å­˜ä½¿ç”¨é‡
        current_memory = sum(exp['memory_required'] for exp in self.running_experiments.values())
        if current_memory + experiment['memory_required'] > self.gpu_memory_limit_mb:
            return False
        
        return True
    
    def start_experiment(self, experiment):
        """å¯åŠ¨å®éªŒ"""
        with self.lock:
            experiment['status'] = 'running'
            experiment['start_time'] = time.time()
            self.running_experiments[experiment['config_name']] = experiment
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå®éªŒ
        thread = threading.Thread(
            target=self._run_experiment_thread,
            args=(experiment,),
            daemon=True
        )
        thread.start()
        
        return experiment
    
    def _run_experiment_thread(self, experiment):
        """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œå®éªŒ"""
        try:
            # è¿è¡Œå®éªŒ
            success, stdout, stderr, duration = self._execute_experiment(experiment)
            
            # æ›´æ–°å®éªŒçŠ¶æ€
            with self.lock:
                experiment['status'] = 'completed' if success else 'failed'
                experiment['end_time'] = time.time()
                experiment['duration'] = duration
                experiment['stdout'] = stdout
                experiment['stderr'] = stderr
                
                # ä»è¿è¡Œåˆ—è¡¨ä¸­ç§»é™¤
                if experiment['config_name'] in self.running_experiments:
                    del self.running_experiments[experiment['config_name']]
                
                # æ·»åŠ åˆ°å®Œæˆåˆ—è¡¨
                if success:
                    self.completed_experiments.append(experiment)
                    print(f"âœ… å®Œæˆ: {experiment['config_name']} ({duration:.1f}s)")
                else:
                    self.failed_experiments.append(experiment)
                    print(f"âŒ å¤±è´¥: {experiment['config_name']} - {stderr[:100]}")
                
        except Exception as e:
            with self.lock:
                experiment['status'] = 'error'
                experiment['error'] = str(e)
                if experiment['config_name'] in self.running_experiments:
                    del self.running_experiments[experiment['config_name']]
                self.failed_experiments.append(experiment)
            print(f"ğŸ’¥ é”™è¯¯: {experiment['config_name']} - {e}")
    
    def _execute_experiment(self, experiment):
        """æ‰§è¡Œå•ä¸ªå®éªŒ"""
        try:
            # åŠ è½½é…ç½®æ–‡ä»¶
            with open(experiment['config_file'], 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # å¼ºåˆ¶ä½¿ç”¨GPUï¼Œç¦ç”¨CPUå›é€€
            config['force_gpu'] = True
            
            # ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„data_path
            config['data_path'] = experiment['data_file']
            config['plant_id'] = experiment['project_id']
            
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            temp_config = f"temp_config_{experiment['project_id']}_{experiment['config_name']}_{int(time.time())}.yaml"
            with open(temp_config, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            # è¿è¡Œå®éªŒ
            cmd = ['python', 'main.py', '--config', temp_config]
            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)
            duration = time.time() - start_time
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_config):
                os.remove(temp_config)
            
            success = result.returncode == 0
            return success, result.stdout, result.stderr, duration
            
        except Exception as e:
            return False, "", str(e), 0.0
    
    def run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨ä¸»å¾ªç¯"""
        print(f"ğŸš€ å¯åŠ¨å•GPUå¹¶è¡Œè°ƒåº¦å™¨ (æœ€å¤§å¹¶è¡Œ: {self.max_parallel})")
        
        while not self.experiment_queue.empty() or self.running_experiments:
            # å°è¯•å¯åŠ¨æ–°å®éªŒ
            experiments_to_start = []
            temp_queue = queue.Queue()
            
            # æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„å®éªŒ
            while not self.experiment_queue.empty():
                experiment = self.experiment_queue.get()
                if self.can_start_experiment(experiment):
                    experiments_to_start.append(experiment)
                    break
                else:
                    temp_queue.put(experiment)
            
            # å°†æœªå¯åŠ¨çš„å®éªŒæ”¾å›é˜Ÿåˆ—
            while not temp_queue.empty():
                self.experiment_queue.put(temp_queue.get())
            
            # å¯åŠ¨æ–°å®éªŒ
            for experiment in experiments_to_start:
                self.start_experiment(experiment)
                current_memory = sum(exp['memory_required'] for exp in self.running_experiments.values())
                print(f"ğŸ”„ å¯åŠ¨: {experiment['config_name']} (å†…å­˜: {current_memory}/{self.gpu_memory_limit_mb}MB)")
            
            # æ˜¾ç¤ºçŠ¶æ€
            status = self.get_status()
            if status['running'] > 0 or status['queued'] > 0:
                print(f"ğŸ“Š é˜Ÿåˆ— {status['queued']} | è¿è¡Œ {status['running']} | å®Œæˆ {status['completed']} | å¤±è´¥ {status['failed']}")
            
            time.sleep(3)  # ç­‰å¾…3ç§’åå†æ¬¡æ£€æŸ¥
        
        print("âœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
    
    def get_status(self):
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        with self.lock:
            return {
                'queued': self.experiment_queue.qsize(),
                'running': len(self.running_experiments),
                'completed': len(self.completed_experiments),
                'failed': len(self.failed_experiments),
                'running_experiments': list(self.running_experiments.keys())
            }

def get_project_configs(project_id):
    """è·å–é¡¹ç›®çš„æ‰€æœ‰é…ç½®æ–‡ä»¶"""
    config_dir = Path(f"config/projects/{project_id}")
    if not config_dir.exists():
        return []
    
    config_files = []
    for config_file in sorted(config_dir.glob("*.yaml")):
        if config_file.name not in ['config_index.yaml']:
            config_files.append(str(config_file))
    
    return config_files

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ SolarPVé¡¹ç›® - å•GPUå¹¶è¡Œå®éªŒç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPU")
        return
    
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
    print(f"ğŸ¯ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # æ ¹æ®GPUå†…å­˜è®¾ç½®å‚æ•°
    if gpu_memory >= 20:  # 20GB+
        max_parallel = 3
        memory_limit = int(gpu_memory * 1024 * 0.8)  # 80%å†…å­˜ä½¿ç”¨ç‡
    elif gpu_memory >= 12:  # 12GB+
        max_parallel = 2
        memory_limit = int(gpu_memory * 1024 * 0.8)
    else:  # 8GB
        max_parallel = 2
        memory_limit = int(gpu_memory * 1024 * 0.7)  # 70%å†…å­˜ä½¿ç”¨ç‡
    
    print(f"ğŸ“Š æœ€å¤§å¹¶è¡Œæ•°: {max_parallel}")
    print(f"ğŸ“Š å†…å­˜é™åˆ¶: {memory_limit}MB")
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = SingleGPUParallelScheduler(max_parallel=max_parallel, gpu_memory_limit_mb=memory_limit)
    
    # è·å–é¡¹ç›®åˆ—è¡¨
    projects = [171, 172, 186]  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    data_dir = "data"
    
    # æ·»åŠ å®éªŒåˆ°é˜Ÿåˆ—
    total_experiments = 0
    for project_id in projects:
        data_file = os.path.join(data_dir, f"Project{project_id}.csv")
        if not os.path.exists(data_file):
            print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            continue
        
        config_files = get_project_configs(project_id)
        print(f"ğŸ“ é¡¹ç›® {project_id}: {len(config_files)} ä¸ªå®éªŒ")
        
        for config_file in config_files:
            scheduler.add_experiment(config_file, data_file, project_id)
            total_experiments += 1
    
    print(f"ğŸ“Š æ€»å®éªŒæ•°: {total_experiments}")
    
    # å¯åŠ¨è°ƒåº¦å™¨
    start_time = time.time()
    scheduler.run_scheduler()
    total_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœ
    status = scheduler.get_status()
    print("\n" + "=" * 60)
    print("ğŸ‰ å•GPUå¹¶è¡Œå®éªŒå®Œæˆ!")
    print(f"ğŸ“Š æ€»å®éªŒæ•°: {total_experiments}")
    print(f"âœ… æˆåŠŸ: {status['completed']}")
    print(f"âŒ å¤±è´¥: {status['failed']}")
    print(f"â±ï¸ æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_experiments/(total_time/3600):.1f} å®éªŒ/å°æ—¶")
    print(f"ğŸ¯ å¹¶è¡Œæ•ˆç‡: {total_experiments/(total_time/3600)/max_parallel:.1f} å®éªŒ/å°æ—¶/å¹¶è¡Œ")

if __name__ == "__main__":
    main()
