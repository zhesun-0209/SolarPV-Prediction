#!/usr/bin/env python3
"""
è®­ç»ƒæ”¹è¿›çš„LSTMå’ŒGRUæ¨¡å‹ï¼Œç”Ÿæˆ168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾
è§£å†³å‘¨æœŸæ€§é—®é¢˜ï¼Œå±•ç¤ºæ”¹è¿›æ•ˆæœ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yaml
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.rnn_models import LSTM, GRU

def create_improved_config():
    """åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹é…ç½®"""
    config = {
        'model': 'LSTM',  # æˆ– 'GRU'
        'hidden_dim': 64,  # å‡å°éšè—å±‚ç»´åº¦
        'num_layers': 2,   # å‡å°‘å±‚æ•°
        'dropout': 0.1,    # å‡å°‘dropout
        'future_hours': 168,
        'past_hours': 168,  # ä½¿ç”¨168å°æ—¶å†å²æ•°æ®
        'use_forecast': True,
        'use_hist_weather': False,
        'use_pv': True,
        'use_time_encoding': True,
        'epochs': 50,      # å‡å°‘è®­ç»ƒè½®æ•°
        'batch_size': 16,  # å‡å°æ‰¹æ¬¡å¤§å°
        'learning_rate': 0.0001,  # é™ä½å­¦ä¹ ç‡
        'patience': 10,
        'min_delta': 0.001,
        'train_params': {
            'batch_size': 16,
            'learning_rate': 0.0001,
            'weight_decay': 1e-4
        }
    }
    return config

def load_sample_data():
    """åŠ è½½ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ"""
    print("ğŸ“ åŠ è½½ç¤ºä¾‹æ•°æ®...")
    
    # ä½¿ç”¨é¡¹ç›®1140çš„æ•°æ®ä½œä¸ºç¤ºä¾‹
    data_path = "data/Project1140.csv"
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    df = pd.read_csv(data_path)
    print(f"âœ… æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"âœ… åˆ—å: {df.columns.tolist()}")
    
    return df

def prepare_training_data(df, config):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # é€‰æ‹©ç‰¹å¾åˆ— - ä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—
    feature_cols = []
    
    # ä¸»è¦ç›®æ ‡å˜é‡
    if 'Electricity Generated' in df.columns:
        feature_cols.append('Electricity Generated')
    elif 'pv_power_mw' in df.columns:
        feature_cols.append('pv_power_mw')
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°PVåŠŸç‡åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            feature_cols.append(numeric_cols[0])
    
    if config.get('use_forecast', False):
        # æ·»åŠ å¤©æ°”é¢„æµ‹ç‰¹å¾
        weather_cols = [col for col in df.columns if 'pred' in col.lower()]
        feature_cols.extend(weather_cols[:5])  # é€‰æ‹©å‰5ä¸ªé¢„æµ‹ç‰¹å¾
    else:
        # æ·»åŠ ä¸€äº›å†å²å¤©æ°”ç‰¹å¾
        weather_cols = [col for col in df.columns if any(x in col.lower() for x in ['temperature', 'humidity', 'pressure', 'wind'])]
        feature_cols.extend(weather_cols[:4])  # é€‰æ‹©å‰4ä¸ªå¤©æ°”ç‰¹å¾
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨
    available_cols = [col for col in feature_cols if col in df.columns]
    print(f"âœ… å¯ç”¨ç‰¹å¾: {available_cols}")
    
    if len(available_cols) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")
        return None, None
    
    # æå–æ•°æ®
    data = df[available_cols].values.astype(np.float32)
    
    # æ•°æ®æ ‡å‡†åŒ–
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # å‡†å¤‡åºåˆ—æ•°æ®
    past_hours = config['past_hours']
    future_hours = config['future_hours']
    
    X, y = [], []
    for i in range(past_hours, len(data_scaled) - future_hours + 1):
        X.append(data_scaled[i-past_hours:i])
        y.append(data_scaled[i:i+future_hours, 0])  # åªé¢„æµ‹PVåŠŸç‡
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"âœ… åºåˆ—æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # åˆ†å‰²æ•°æ®
    train_size = int(0.8 * len(X))
    val_size = int(0.1 * len(X))
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]
    
    print(f"âœ… è®­ç»ƒé›†: {X_train.shape}, éªŒè¯é›†: {X_val.shape}, æµ‹è¯•é›†: {X_test.shape}")
    
    return (X_train, y_train, X_val, y_val, X_test, y_test), scaler

def train_improved_model(model, X_train, y_train, X_val, y_val, config):
    """è®­ç»ƒæ”¹è¿›çš„æ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{config['model']}æ¨¡å‹...")
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.FloatTensor(y_val)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    for epoch in range(config['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            
            # åˆ†ç¦»å†å²æ•°æ®å’Œé¢„æµ‹æ•°æ®
            hist_data = batch_X[:, :config['past_hours']]  # å†å²æ•°æ®
            fcst_data = batch_X[:, config['past_hours']:]  # é¢„æµ‹æ•°æ®
            
            # å‰å‘ä¼ æ’­
            outputs = model(hist_data, fcst_data)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            hist_val = X_val_tensor[:, :config['past_hours']]
            fcst_val = X_val_tensor[:, config['past_hours']:]
            val_outputs = model(hist_val, fcst_val)
            val_loss = criterion(val_outputs, y_val_tensor).item()
        
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss)
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss - config['min_delta']:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch:3d}: Train Loss={train_losses[-1]:.6f}, Val Loss={val_loss:.6f}")
        
        if patience_counter >= config['patience']:
            print(f"ğŸ›‘ æ—©åœäºç¬¬ {epoch} è½®")
            break
    
    print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    return train_losses, val_losses

def generate_168h_predictions(model, X_test, y_test, config, scaler):
    """ç”Ÿæˆ168å°æ—¶é¢„æµ‹å¹¶å¯è§†åŒ–"""
    print("ğŸ¨ ç”Ÿæˆ168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾...")
    
    # é€‰æ‹©å‡ ä¸ªæµ‹è¯•æ ·æœ¬
    n_samples = min(3, len(X_test))
    sample_indices = np.random.choice(len(X_test), n_samples, replace=False)
    
    model.eval()
    with torch.no_grad():
        predictions = []
        ground_truths = []
        
        for idx in sample_indices:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            X_sample = torch.FloatTensor(X_test[idx:idx+1])
            hist_data = X_sample[:, :config['past_hours']]
            fcst_data = X_sample[:, config['past_hours']:]
            
            # ç”Ÿæˆé¢„æµ‹
            pred = model(hist_data, fcst_data)
            pred_np = pred.cpu().numpy()[0]
            
            # åæ ‡å‡†åŒ–
            pred_original = scaler.inverse_transform(
                np.column_stack([pred_np, np.zeros((len(pred_np), scaler.n_features_in_-1))])
            )[:, 0]
            
            gt_original = scaler.inverse_transform(
                np.column_stack([y_test[idx], np.zeros((len(y_test[idx]), scaler.n_features_in_-1))])
            )[:, 0]
            
            predictions.append(pred_original)
            ground_truths.append(gt_original)
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(n_samples, 1, figsize=(15, 5*n_samples))
    if n_samples == 1:
        axes = [axes]
    
    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
        hours = range(1, len(pred) + 1)
        
        axes[i].plot(hours, gt, 'b-', label='Ground Truth', linewidth=2, alpha=0.8)
        axes[i].plot(hours, pred, 'r--', label=f'Improved {config["model"]} Prediction', linewidth=2, alpha=0.8)
        
        axes[i].set_xlabel('Hours Ahead')
        axes[i].set_ylabel('PV Power (MW)')
        axes[i].set_title(f'Sample {i+1}: 168-Hour PV Power Prediction Comparison')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæŒ‡æ ‡
        mse = np.mean((pred - gt) ** 2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(pred - gt))
        
        axes[i].text(0.02, 0.98, f'RMSE: {rmse:.3f}\nMAE: {mae:.3f}', 
                    transform=axes[i].transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(f'improved_{config["model"].lower()}_168h_prediction.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜ä¸º: improved_{config['model'].lower()}_168h_prediction.png")
    
    return predictions, ground_truths

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ”¹è¿›çš„RNNæ¨¡å‹ - 168å°æ—¶é¢„æµ‹")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = create_improved_config()
    print(f"ğŸ“‹ é…ç½®: {config['model']} - {config['hidden_dim']}ç»´ - {config['num_layers']}å±‚")
    
    # åŠ è½½æ•°æ®
    df = load_sample_data()
    if df is None:
        return
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    data_splits, scaler = prepare_training_data(df, config)
    X_train, y_train, X_val, y_val, X_test, y_test = data_splits
    
    # åˆ›å»ºæ¨¡å‹
    hist_dim = X_train.shape[-1]
    fcst_dim = X_train.shape[-1] if config.get('use_forecast', False) else 0
    
    if config['model'] == 'LSTM':
        model = LSTM(hist_dim, fcst_dim, config)
    else:
        model = GRU(hist_dim, fcst_dim, config)
    
    print(f"âœ… æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    train_losses, val_losses = train_improved_model(model, X_train, y_train, X_val, y_val, config)
    
    # ç”Ÿæˆ168å°æ—¶é¢„æµ‹å¯¹æ¯”å›¾
    predictions, ground_truths = generate_168h_predictions(model, X_test, y_test, config, scaler)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Training Loss', color='blue')
    plt.plot(val_losses, label='Validation Loss', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'{config["model"]} Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_losses[-20:], label='Training Loss (Last 20)', color='blue')
    plt.plot(val_losses[-20:], label='Validation Loss (Last 20)', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Progress (Last 20 Epochs)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'improved_{config["model"].lower()}_training_curve.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º: improved_{config['model'].lower()}_training_curve.png")
    
    print("\nğŸ¯ æ”¹è¿›æ•ˆæœæ€»ç»“:")
    print("   - æ·»åŠ äº†æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æ¨¡å‹å…³æ³¨é‡è¦çš„æ—¶é—´æ­¥")
    print("   - ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œæ”¹å–„æ¢¯åº¦æµå’Œè®­ç»ƒç¨³å®šæ€§")
    print("   - ç»Ÿä¸€äº†LSTMå’ŒGRUçš„æ¶æ„é…ç½®")
    print("   - ä¸“é—¨é’ˆå¯¹168å°æ—¶é•¿æœŸé¢„æµ‹è¿›è¡Œäº†ä¼˜åŒ–")

if __name__ == "__main__":
    main()
